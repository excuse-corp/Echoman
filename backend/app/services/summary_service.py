"""
æ‘˜è¦ç”ŸæˆæœåŠ¡

å®ç°å¢é‡æ‘˜è¦ç”Ÿæˆï¼Œé¿å…æ¯æ¬¡éƒ½å¯¹æ•´ä¸ªä¸»é¢˜é‡æ–°ç”Ÿæˆæ‘˜è¦
é‡‡ç”¨å…³é”®èŠ‚ç‚¹é€‰æ‹© + å¢é‡åˆæˆçš„ç­–ç•¥

ä¼˜åŒ–ï¼šé›†æˆ TokenManager ä»¥å¤„ç† qwen3-32b çš„ 32k ä¸Šä¸‹æ–‡é™åˆ¶
"""
from typing import List, Optional, Dict
from datetime import datetime
from datetime import timedelta
from app.utils.timezone import now_cn
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
import json
import logging

from app.models import Summary, Topic, TopicNode, SourceItem, LLMJudgement
from app.services.llm.factory import get_llm_provider
from app.config import settings
from app.utils.token_manager import get_token_manager

logger = logging.getLogger(__name__)


class SummaryService:
    """æ‘˜è¦ç”ŸæˆæœåŠ¡"""
    
    def __init__(self):
        self.settings = settings
        self.llm_provider = get_llm_provider()
        self.token_manager = get_token_manager(model=settings.qwen_model)
        self.min_nodes_for_update = 3  # æœ€å°‘æ–°èŠ‚ç‚¹æ•°æ‰è§¦å‘æ›´æ–°
        self.max_context_nodes = 15  # æœ€å¤šä½¿ç”¨çš„èŠ‚ç‚¹æ•°
        self.update_interval_hours = 6  # æ›´æ–°é—´éš”ï¼ˆå°æ—¶ï¼‰
        # Token é™åˆ¶ï¼šä¸ºæ‘˜è¦ç”Ÿæˆé¢„ç•™åˆç†çš„ token é¢„ç®—
        self.max_prompt_tokens = 4000  # è¾“å…¥ä¸Šä¸‹æ–‡æœ€å¤§ tokenï¼ˆè€ƒè™‘åˆ°æ‘˜è¦ä»»åŠ¡è¾ƒå¤æ‚ï¼‰
        self.max_completion_tokens = 1000  # æ‘˜è¦æœ€å¤§ token
        
    async def generate_or_update_summary(
        self,
        db: AsyncSession,
        topic: Topic,
        new_nodes: Optional[List[TopicNode]] = None
    ) -> Optional[Summary]:
        """
        ç”Ÿæˆæˆ–æ›´æ–°ä¸»é¢˜æ‘˜è¦
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            topic: ä¸»é¢˜å¯¹è±¡
            new_nodes: æ–°å¢çš„èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå¦‚æœæ˜¯å¢é‡æ›´æ–°ï¼‰
            
        Returns:
            Summaryå¯¹è±¡ï¼Œå¦‚æœæ— éœ€æ›´æ–°åˆ™è¿”å›None
        """
        # è·å–å½“å‰æ‘˜è¦
        current_summary = await self._get_current_summary(db, topic.id)
        
        if current_summary is None:
            # é¦–æ¬¡ç”Ÿæˆå…¨é‡æ‘˜è¦
            return await self.generate_full_summary(db, topic)
        else:
            # å¢é‡æ›´æ–°
            return await self.generate_incremental_summary(
                db, 
                topic, 
                current_summary,
                new_nodes or []
            )
    
    async def generate_full_summary(
        self,
        db: AsyncSession,
        topic: Topic
    ) -> Summary:
        """
        ç”Ÿæˆå…¨é‡æ‘˜è¦
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            topic: ä¸»é¢˜å¯¹è±¡
            
        Returns:
            Summaryå¯¹è±¡
        """
        logger.info(f"ğŸ”„ å¼€å§‹ç”Ÿæˆå…¨é‡æ‘˜è¦ - Topic ID: {topic.id}, æ ‡é¢˜: {topic.title_key}")
        
        # 1. è·å–æ‰€æœ‰èŠ‚ç‚¹
        all_nodes = await self._get_all_topic_nodes(db, topic.id)
        logger.info(f"   è·å–åˆ° {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
        
        if not all_nodes:
            # æ— èŠ‚ç‚¹ï¼Œåˆ›å»ºå ä½æ‘˜è¦
            logger.warning(f"   âš ï¸  æ— èŠ‚ç‚¹ï¼Œåˆ›å»ºå ä½æ‘˜è¦")
            return await self._create_placeholder_summary(db, topic)
        
        # 2. é€‰æ‹©å…³é”®èŠ‚ç‚¹
        key_nodes = self._select_key_nodes(all_nodes)
        logger.info(f"   é€‰æ‹©äº† {len(key_nodes)} ä¸ªå…³é”®èŠ‚ç‚¹")
        
        # 3. è·å–ä¸»é¢˜ç»Ÿè®¡ä¿¡æ¯
        stats = await self._get_topic_stats(db, topic)
        logger.info(f"   ç»Ÿè®¡: {stats['node_count']} ä¸ªèŠ‚ç‚¹, å¹³å°: {stats['platforms']}")
        
        # 4. æ„é€ Promptï¼ˆå¸¦ Token ä¼˜åŒ–ï¼‰
        prompt = self._build_full_prompt(topic, key_nodes, stats)
        
        # 5. Token ä¼˜åŒ–ï¼šç¡®ä¿ prompt ä¸è¶…è¿‡é™åˆ¶
        prompt_tokens = self.token_manager.count_tokens(prompt)
        logger.info(f"   Prompt tokens: {prompt_tokens}")
        
        if prompt_tokens > self.max_prompt_tokens:
            logger.warning(
                f"å…¨é‡æ‘˜è¦ prompt è¿‡é•¿ ({prompt_tokens} tokens)ï¼Œéœ€è¦æˆªæ–­ä¸Šä¸‹æ–‡"
            )
            # æˆªæ–­ promptï¼ˆä¿ç•™ç³»ç»Ÿæç¤ºå’Œä¸»é¢˜ä¿¡æ¯ï¼Œå‹ç¼©èŠ‚ç‚¹å†…å®¹ï¼‰
            prompt = self.token_manager.truncate_text(
                prompt, 
                max_tokens=self.max_prompt_tokens
            )
            logger.info(f"æˆªæ–­å prompt: {self.token_manager.count_tokens(prompt)} tokens")
        
        # 6. è°ƒç”¨LLM
        logger.info(f"   ğŸ“¡ è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦ (provider: {self.llm_provider.get_provider_name()}, model: {self.llm_provider.model})")
        try:
            response = await self.llm_provider.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=self.max_completion_tokens  # ä½¿ç”¨é…ç½®çš„å€¼ï¼ˆ1000ï¼‰
            )
            logger.info(f"   âœ… LLMè°ƒç”¨æˆåŠŸ")
            
            # 7. è§£æå“åº”
            summary_data = self._parse_summary_response(response)
            logger.info(f"   âœ… å“åº”è§£ææˆåŠŸï¼Œæ‘˜è¦é•¿åº¦: {len(summary_data.get('summary', ''))} å­—")
            
            # 8. è®°å½• Token ä½¿ç”¨
            completion_tokens = response.get('usage', {}).get('completion_tokens', 0) if isinstance(response, dict) else 0
            logger.info(
                f"   ğŸ“Š Tokenç»Ÿè®¡ - Prompt: {prompt_tokens}, Completion: {completion_tokens}"
            )
            
            # 9. ä¿å­˜æ‘˜è¦
            summary = Summary(
                topic_id=topic.id,
                content=summary_data["summary"],
                method="full",
                provider=self.llm_provider.get_provider_name(),
                model=self.llm_provider.model,
                generated_at=now_cn()
            )
            db.add(summary)
            await db.commit()
            await db.refresh(summary)
            logger.info(f"   ğŸ’¾ æ‘˜è¦å·²ä¿å­˜åˆ°æ•°æ®åº“ (Summary ID: {summary.id})")
            
            # 10. æ›´æ–°topicçš„summary_id
            topic.summary_id = summary.id
            await db.commit()
            logger.info(f"   ğŸ”— å·²å…³è”åˆ°Topic")
            
            # 11. è®°å½•åˆ¤å®šä»»åŠ¡
            await self._record_judgement(
                db,
                topic_id=topic.id,
                type="summarize_full",
                prompt=prompt,
                response=response,
                summary_data=summary_data
            )
            
            logger.info(f"âœ… å…¨é‡æ‘˜è¦ç”Ÿæˆå®Œæˆ - Topic ID: {topic.id}")
            return summary
            
        except Exception as e:
            logger.error(f"å…¨é‡æ‘˜è¦ç”Ÿæˆå¤±è´¥ - Topic ID: {topic.id}, æ ‡é¢˜: {topic.title_key}")
            logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            logger.error(f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
            print(f"âŒ å…¨é‡æ‘˜è¦ç”Ÿæˆå¤±è´¥ (Topic {topic.id}): {e}")
            return await self._create_placeholder_summary(db, topic)
    
    async def generate_incremental_summary(
        self,
        db: AsyncSession,
        topic: Topic,
        current_summary: Summary,
        new_nodes: List[TopicNode]
    ) -> Optional[Summary]:
        """
        ç”Ÿæˆå¢é‡æ‘˜è¦
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            topic: ä¸»é¢˜å¯¹è±¡
            current_summary: å½“å‰æ‘˜è¦
            new_nodes: æ–°å¢èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            æ–°çš„Summaryå¯¹è±¡ï¼Œå¦‚æœæ— éœ€æ›´æ–°åˆ™è¿”å›None
        """
        # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not await self._should_update(current_summary, new_nodes):
            return None
        
        # 2. å‹ç¼©æ–°èŠ‚ç‚¹ï¼ˆå¦‚æœå¤ªå¤šï¼‰
        compressed_nodes = self._compress_new_nodes(new_nodes)
        
        # 3. æ„é€ å¢é‡Promptï¼ˆå¸¦ Token ä¼˜åŒ–ï¼‰
        prompt = self._build_incremental_prompt(
            topic,
            current_summary.content,
            compressed_nodes
        )
        
        # 4. Token ä¼˜åŒ–ï¼šç¡®ä¿ prompt ä¸è¶…è¿‡é™åˆ¶
        prompt_tokens = self.token_manager.count_tokens(prompt)
        if prompt_tokens > self.max_prompt_tokens:
            logger.warning(
                f"å¢é‡æ‘˜è¦ prompt è¿‡é•¿ ({prompt_tokens} tokens)ï¼Œéœ€è¦æˆªæ–­"
            )
            # æˆªæ–­ promptï¼ˆä¼˜å…ˆä¿ç•™å½“å‰æ‘˜è¦å’Œæ–°èŠ‚ç‚¹æ‘˜è¦ï¼‰
            prompt = self.token_manager.truncate_text(
                prompt,
                max_tokens=self.max_prompt_tokens
            )
            logger.info(f"æˆªæ–­å prompt: {self.token_manager.count_tokens(prompt)} tokens")
        
        # 5. è°ƒç”¨LLM
        try:
            response = await self.llm_provider.chat_completion(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=self.max_completion_tokens  # ä½¿ç”¨é…ç½®çš„å€¼ï¼ˆ1000ï¼‰
            )
            
            # 6. è§£æå“åº”
            update_data = self._parse_incremental_response(response)
            
            # 7. è®°å½• Token ä½¿ç”¨
            logger.info(
                f"å¢é‡æ‘˜è¦ç”Ÿæˆå®Œæˆ - Prompt: {prompt_tokens} tokens, "
                f"Completion: {response.get('usage', {}).get('completion_tokens', 0)} tokens"
            )
            
            # 8. å¦‚æœLLMåˆ¤æ–­ä¸éœ€è¦æ›´æ–°
            if not update_data.get("needs_update", True):
                return None
            
            # 9. ä¿å­˜æ–°æ‘˜è¦
            summary = Summary(
                topic_id=topic.id,
                content=update_data["updated_summary"],
                method="incremental",
                provider=self.llm_provider.get_provider_name(),
                model=self.llm_provider.model,
                generated_at=now_cn()
            )
            db.add(summary)
            await db.commit()
            await db.refresh(summary)
            
            # 10. æ›´æ–°topicçš„summary_id
            topic.summary_id = summary.id
            await db.commit()
            
            # 11. è®°å½•åˆ¤å®šä»»åŠ¡
            await self._record_judgement(
                db,
                topic_id=topic.id,
                type="summarize_incremental",
                prompt=prompt,
                response=response,
                summary_data=update_data
            )
            
            return summary
            
        except Exception as e:
            logger.error(f"å¢é‡æ‘˜è¦ç”Ÿæˆå¤±è´¥ - Topic ID: {topic.id}, æ ‡é¢˜: {topic.title_key}")
            logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            logger.error(f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
            print(f"âŒ å¢é‡æ‘˜è¦ç”Ÿæˆå¤±è´¥ (Topic {topic.id}): {e}")
            return None
    
    def _select_key_nodes(self, nodes: List[TopicNode]) -> List[TopicNode]:
        """
        é€‰æ‹©å…³é”®èŠ‚ç‚¹ä»¥å‹ç¼©ä¸Šä¸‹æ–‡
        
        ç­–ç•¥ï¼šé¦–æ¡ + å³°å€¼ï¼ˆäº’åŠ¨æœ€é«˜ï¼‰+ æœ€æ–°
        """
        if not nodes:
            return []
        
        key_nodes = []
        seen_ids = set()
        
        # 1. é¦–æ¡èŠ‚ç‚¹ï¼ˆäº‹ä»¶èµ·å› ï¼‰
        first_node = min(nodes, key=lambda n: n.appended_at)
        key_nodes.append(first_node)
        seen_ids.add(first_node.id)
        
        # 2. å³°å€¼èŠ‚ç‚¹ï¼ˆäº’åŠ¨é‡æœ€é«˜çš„å‰2æ¡ï¼‰
        nodes_with_interactions = [
            n for n in nodes 
            if n.source_item and n.source_item.interactions
        ]
        
        if nodes_with_interactions:
            sorted_by_interactions = sorted(
                nodes_with_interactions,
                key=lambda n: self._get_total_interactions(n),
                reverse=True
            )
            
            for node in sorted_by_interactions[:2]:
                if node.id not in seen_ids:
                    key_nodes.append(node)
                    seen_ids.add(node.id)
        
        # 3. æœ€æ–°èŠ‚ç‚¹ï¼ˆæœ€è¿‘çš„5æ¡ï¼‰
        latest_nodes = sorted(nodes, key=lambda n: n.appended_at, reverse=True)[:5]
        for node in latest_nodes:
            if node.id not in seen_ids:
                key_nodes.append(node)
                seen_ids.add(node.id)
        
        # æŒ‰æ—¶é—´æ’åº
        key_nodes.sort(key=lambda n: n.appended_at)
        
        # é™åˆ¶æ€»æ•°
        return key_nodes[:self.max_context_nodes]
    
    def _compress_new_nodes(self, nodes: List[TopicNode]) -> List[TopicNode]:
        """å‹ç¼©æ–°èŠ‚ç‚¹åˆ—è¡¨"""
        if len(nodes) <= 5:
            return nodes
        
        # ä¿ç•™æœ€æ–°çš„5æ¡
        return sorted(nodes, key=lambda n: n.appended_at, reverse=True)[:5]
    
    def _build_full_prompt(
        self, 
        topic: Topic, 
        key_nodes: List[TopicNode],
        stats: Dict
    ) -> str:
        """æ„é€ å…¨é‡æ‘˜è¦Prompt"""
        nodes_text = []
        for i, node in enumerate(key_nodes, 1):
            if node.source_item:
                # ä½¿ç”¨published_atï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨fetched_at
                pub_time = node.source_item.published_at or node.source_item.fetched_at
                time_str = pub_time.strftime("%Y-%m-%d %H:%M") if pub_time else "æœªçŸ¥æ—¶é—´"
                nodes_text.append(
                    f"{i}. [{node.source_item.platform}] {time_str}\n"
                    f"   æ ‡é¢˜: {node.source_item.title}"
                )
                if node.source_item.summary:
                    nodes_text.append(f"   æ‘˜è¦: {node.source_item.summary[:150]}")
                
                # äº’åŠ¨æ•°æ®
                interactions = self._format_interactions(node.source_item.interactions)
                if interactions:
                    nodes_text.append(f"   äº’åŠ¨: {interactions}")
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹çƒ­ç‚¹äº‹ä»¶ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ã€‚

ã€äº‹ä»¶åŸºæœ¬ä¿¡æ¯ã€‘
- æ ‡é¢˜: {topic.title_key}
- é¦–æ¬¡å‘ç°: {topic.first_seen.strftime("%Y-%m-%d %H:%M") if topic.first_seen else "æœªçŸ¥"}
- æœ€åæ´»è·ƒ: {topic.last_active.strftime("%Y-%m-%d %H:%M") if topic.last_active else "æœªçŸ¥"}
- æ¶‰åŠå¹³å°: {stats['platforms']}
- èŠ‚ç‚¹æ€»æ•°: {stats['node_count']}

ã€å…³é”®èŠ‚ç‚¹ã€‘ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼Œå·²ç­›é€‰å…³é”®ä¿¡æ¯ï¼‰
{chr(10).join(nodes_text)}

è¦æ±‚ï¼š
1. æ¦‚è¿°äº‹ä»¶çš„æ ¸å¿ƒå†…å®¹ï¼ˆ150-300å­—ï¼‰
2. æç‚¼3-5ä¸ªå…³é”®è¦ç‚¹
3. å¦‚æœæœ‰é‡è¦è¿›å±•ï¼ŒæŒ‰æ—¶é—´é¡ºåºè¯´æ˜
4. ä¿æŒå®¢è§‚ä¸­ç«‹ï¼Œä¸åšä¸»è§‚è¯„ä»·

é‡è¦ï¼šç›´æ¥è¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€ç»´è¿‡ç¨‹æˆ–å…¶ä»–æ–‡æœ¬ã€‚
è¾“å‡ºæ ¼å¼ï¼š
{{
  "summary": "äº‹ä»¶æ¦‚è¿°ï¼ˆ150-300å­—ï¼‰",
  "key_points": [
    "è¦ç‚¹1ï¼šäº‹ä»¶èµ·å› æˆ–èƒŒæ™¯",
    "è¦ç‚¹2ï¼šä¸»è¦å†…å®¹æˆ–è¿›å±•",
    "è¦ç‚¹3ï¼šå½“å‰çŠ¶æ€æˆ–å½±å“"
  ]
}}
"""
        return prompt
    
    def _build_incremental_prompt(
        self,
        topic: Topic,
        current_summary: str,
        new_nodes: List[TopicNode]
    ) -> str:
        """æ„é€ å¢é‡æ‘˜è¦Prompt"""
        new_nodes_text = []
        for i, node in enumerate(new_nodes, 1):
            if node.source_item:
                # ä½¿ç”¨published_atï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨fetched_at
                pub_time = node.source_item.published_at or node.source_item.fetched_at
                time_str = pub_time.strftime("%Y-%m-%d %H:%M") if pub_time else "æœªçŸ¥æ—¶é—´"
                new_nodes_text.append(
                    f"{i}. [{node.source_item.platform}] {time_str}\n"
                    f"   {node.source_item.title}"
                )
                if node.source_item.summary:
                    new_nodes_text.append(f"   {node.source_item.summary[:150]}")
        
        prompt = f"""è¯·åŸºäºå½“å‰æ‘˜è¦å’Œæ–°å¢è¿›å±•ï¼Œæ›´æ–°äº‹ä»¶æ‘˜è¦ã€‚

ã€å½“å‰æ‘˜è¦ã€‘
{current_summary}

ã€æ–°å¢è¿›å±•ã€‘ï¼ˆ{len(new_nodes)}æ¡æ–°èŠ‚ç‚¹ï¼‰
{chr(10).join(new_nodes_text)}

è¯·åˆ†ææ–°å¢å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ‘˜è¦ã€‚

æ›´æ–°åŸåˆ™ï¼š
1. å¦‚æœæ–°èŠ‚ç‚¹åªæ˜¯é‡å¤æ—§ä¿¡æ¯ï¼Œä¿æŒåŸæ‘˜è¦ä¸å˜
2. å¦‚æœæœ‰é‡è¦æ–°è¿›å±•æˆ–è½¬æŠ˜ï¼Œæ›´æ–°æ‘˜è¦å¹¶æ·»åŠ æ–°è¦ç‚¹
3. ä¿æŒæ‘˜è¦ç®€æ´ï¼ˆ150-300å­—ï¼‰
4. ä¿ç•™å†å²æ‘˜è¦çš„è¿è´¯æ€§

é‡è¦ï¼šç›´æ¥è¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€ç»´è¿‡ç¨‹æˆ–å…¶ä»–æ–‡æœ¬ã€‚
è¾“å‡ºæ ¼å¼ï¼š
{{
  "needs_update": true,
  "updated_summary": "æ›´æ–°åçš„æ‘˜è¦ï¼ˆå¦‚æœneeds_update=trueï¼‰",
  "new_key_points": ["æ–°å¢è¦ç‚¹1", "æ–°å¢è¦ç‚¹2"],
  "change_reason": "è¯´æ˜ä¸ºä»€ä¹ˆéœ€è¦æ›´æ–°ï¼ˆæˆ–ä¸ºä»€ä¹ˆä¸éœ€è¦ï¼‰"
}}
"""
        return prompt
    
    def _parse_summary_response(self, response) -> Dict:
        """è§£ææ‘˜è¦å“åº”"""
        try:
            # å¦‚æœresponseæ˜¯dictï¼ˆæ¥è‡ªLLM providerï¼‰ï¼Œæå–contentå­—æ®µ
            if isinstance(response, dict):
                content = response.get("content", "")
            else:
                content = response
            
            # å¦‚æœcontentä¸ºç©ºï¼Œè®°å½•å¹¶ä½¿ç”¨é»˜è®¤å€¼
            if not content:
                logger.error(f"LLMå“åº”contentä¸ºç©º: {response}")
                return {
                    "summary": "æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼šLLMè¿”å›ç©ºå†…å®¹",
                    "key_points": []
                }
            
            # å¤„ç†Qwenæ€ç»´é“¾ï¼šæå–<think>æ ‡ç­¾ä¹‹åçš„å†…å®¹
            content_clean = self._extract_content_from_think(content)
            
            # å°è¯•è§£æJSON
            data = json.loads(content_clean)
            
            if "summary" not in data:
                raise ValueError("Missing summary field")
            
            logger.info(f"   âœ… æˆåŠŸè§£æJSONæ ¼å¼æ‘˜è¦")
            return data
            
        except json.JSONDecodeError as e:
            # é™çº§ï¼šå°è¯•æŸ¥æ‰¾JSONéƒ¨åˆ†
            logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†: {e}")
            
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–JSON
            json_data = self._extract_json_from_text(content_clean if 'content_clean' in locals() else content)
            if json_data:
                logger.info(f"   âœ… ä»æ–‡æœ¬ä¸­æˆåŠŸæå–JSON")
                return json_data
            
            # æœ€ç»ˆé™çº§ï¼šä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºæ‘˜è¦
            logger.warning(f"   âš ï¸  æ— æ³•æå–JSONï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
            fallback_text = content_clean if 'content_clean' in locals() else (content if isinstance(content, str) else str(response))
            return {
                "summary": fallback_text[:500],
                "key_points": []
            }
        except Exception as e:
            logger.error(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
            return {
                "summary": f"æ‘˜è¦è§£æå¤±è´¥ï¼š{str(e)}",
                "key_points": []
            }
    
    def _extract_content_from_think(self, text: str) -> str:
        """ä»Qwenæ€ç»´é“¾è¾“å‡ºä¸­æå–å®é™…å†…å®¹"""
        import re
        
        # å¦‚æœåŒ…å«<think>æ ‡ç­¾ï¼Œæå–</think>ä¹‹åçš„å†…å®¹
        if "<think>" in text.lower() or "</think>" in text.lower():
            logger.info("   æ£€æµ‹åˆ°æ€ç»´é“¾æ ‡ç­¾ï¼Œæå–å®é™…å†…å®¹")
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–</think>ä¹‹åçš„å†…å®¹
            match = re.search(r'</think>\s*(.+)', text, re.DOTALL | re.IGNORECASE)
            if match:
                content = match.group(1).strip()
                logger.info(f"   æå–åˆ°å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                return content
            else:
                # å¦‚æœæ²¡æœ‰</think>ï¼Œå°è¯•æå–<think>ä¹‹å‰çš„å†…å®¹
                match = re.search(r'^(.+?)<think>', text, re.DOTALL | re.IGNORECASE)
                if match:
                    return match.group(1).strip()
                # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›åŸæ–‡
                logger.warning("   æ— æ³•æå–æ€ç»´é“¾åçš„å†…å®¹ï¼Œè¿”å›åŸæ–‡")
                return text
        
        return text
    
    def _extract_json_from_text(self, text: str) -> Optional[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–JSONå¯¹è±¡"""
        import re
        
        # å°è¯•æ‰¾åˆ°JSONå¯¹è±¡ï¼ˆä»¥{å¼€å¤´ï¼Œ}ç»“å°¾ï¼‰
        match = re.search(r'\{[^{}]*"summary"[^{}]*\}', text, re.DOTALL)
        if match:
            try:
                json_str = match.group(0)
                data = json.loads(json_str)
                if "summary" in data:
                    return data
            except json.JSONDecodeError:
                pass
        
        # å°è¯•æ›´å®½æ¾çš„åŒ¹é…
        match = re.search(r'\{.+\}', text, re.DOTALL)
        if match:
            try:
                json_str = match.group(0)
                data = json.loads(json_str)
                if "summary" in data:
                    return data
            except json.JSONDecodeError:
                pass
        
        return None
    
    def _parse_incremental_response(self, response) -> Dict:
        """è§£æå¢é‡å“åº”"""
        try:
            # å¦‚æœresponseæ˜¯dictï¼ˆæ¥è‡ªLLM providerï¼‰ï¼Œæå–contentå­—æ®µ
            if isinstance(response, dict):
                content = response.get("content", "")
            else:
                content = response
            
            # å¦‚æœcontentä¸ºç©ºï¼Œè®°å½•å¹¶ä½¿ç”¨é»˜è®¤å€¼
            if not content:
                logger.error(f"LLMå“åº”contentä¸ºç©º: {response}")
                return {
                    "needs_update": False,
                    "updated_summary": "",
                    "new_key_points": [],
                    "change_reason": "LLMè¿”å›ç©ºå†…å®¹"
                }
            
            # å¤„ç†Qwenæ€ç»´é“¾ï¼šæå–<think>æ ‡ç­¾ä¹‹åçš„å†…å®¹
            content_clean = self._extract_content_from_think(content)
            
            # å°è¯•è§£æJSON
            data = json.loads(content_clean)
            
            # è®¾ç½®é»˜è®¤å€¼
            data.setdefault("needs_update", True)
            
            logger.info(f"   âœ… æˆåŠŸè§£æJSONæ ¼å¼å¢é‡å“åº”")
            return data
            
        except json.JSONDecodeError as e:
            # é™çº§ï¼šå°è¯•æŸ¥æ‰¾JSONéƒ¨åˆ†
            logger.warning(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†: {e}")
            
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–JSON
            json_data = self._extract_json_from_text(content_clean if 'content_clean' in locals() else content)
            if json_data:
                # ç¡®ä¿æœ‰needs_updateå­—æ®µ
                json_data.setdefault("needs_update", True)
                logger.info(f"   âœ… ä»æ–‡æœ¬ä¸­æˆåŠŸæå–JSON")
                return json_data
            
            # æœ€ç»ˆé™çº§ï¼šä½¿ç”¨åŸå§‹æ–‡æœ¬ä½œä¸ºæ‘˜è¦
            logger.warning(f"   âš ï¸  æ— æ³•æå–JSONï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")
            fallback_text = content_clean if 'content_clean' in locals() else (content if isinstance(content, str) else str(response))
            return {
                "needs_update": True,
                "updated_summary": fallback_text[:500],
                "new_key_points": [],
                "change_reason": "Parsed from text"
            }
        except Exception as e:
            logger.error(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
            return {
                "needs_update": False,
                "updated_summary": f"æ‘˜è¦è§£æå¤±è´¥ï¼š{str(e)}",
                "new_key_points": [],
                "change_reason": f"Error: {str(e)}"
            }
    
    async def _should_update(
        self, 
        current_summary: Summary, 
        new_nodes: List[TopicNode]
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ‘˜è¦"""
        # 1. æ£€æŸ¥æ–°èŠ‚ç‚¹æ•°é‡
        if len(new_nodes) < self.min_nodes_for_update:
            return False
        
        # 2. æ£€æŸ¥æ›´æ–°é—´éš”
        time_since_last = now_cn() - current_summary.generated_at
        if time_since_last < timedelta(hours=self.update_interval_hours):
            return False
        
        return True
    
    async def _get_current_summary(
        self, 
        db: AsyncSession, 
        topic_id: int
    ) -> Optional[Summary]:
        """è·å–å½“å‰æ‘˜è¦"""
        stmt = (
            select(Summary)
            .where(Summary.topic_id == topic_id)
            .order_by(Summary.generated_at.desc())
            .limit(1)
        )
        result = await db.execute(stmt)
        return result.scalar_one_or_none()
    
    async def _get_all_topic_nodes(
        self, 
        db: AsyncSession, 
        topic_id: int
    ) -> List[TopicNode]:
        """è·å–ä¸»é¢˜çš„æ‰€æœ‰èŠ‚ç‚¹"""
        from sqlalchemy.orm import joinedload
        
        stmt = (
            select(TopicNode)
            .options(joinedload(TopicNode.source_item))
            .where(TopicNode.topic_id == topic_id)
            .order_by(TopicNode.appended_at.asc())
        )
        result = await db.execute(stmt)
        return list(result.scalars().all())
    
    async def _get_topic_stats(self, db: AsyncSession, topic: Topic) -> Dict:
        """è·å–ä¸»é¢˜ç»Ÿè®¡ä¿¡æ¯"""
        # ç»Ÿè®¡èŠ‚ç‚¹æ•°
        node_count_stmt = (
            select(func.count(TopicNode.id))
            .where(TopicNode.topic_id == topic.id)
        )
        node_count = await db.scalar(node_count_stmt) or 0
        
        # ç»Ÿè®¡å¹³å°æ•°
        platforms_stmt = (
            select(func.array_agg(func.distinct(SourceItem.platform)))
            .select_from(TopicNode)
            .join(SourceItem)
            .where(TopicNode.topic_id == topic.id)
        )
        platforms_result = await db.scalar(platforms_stmt)
        platforms = ", ".join(platforms_result) if platforms_result else "æœªçŸ¥"
        
        return {
            "node_count": node_count,
            "platforms": platforms
        }
    
    async def _create_placeholder_summary(
        self, 
        db: AsyncSession, 
        topic: Topic
    ) -> Summary:
        """åˆ›å»ºå ä½æ‘˜è¦"""
        summary = Summary(
            topic_id=topic.id,
            content=f"äº‹ä»¶ã€Œ{topic.title_key}ã€çš„æ‘˜è¦æ­£åœ¨ç”Ÿæˆä¸­...",
            method="placeholder",
            provider="system",
            model="",
            generated_at=now_cn()
        )
        db.add(summary)
        await db.commit()
        await db.refresh(summary)
        
        topic.summary_id = summary.id
        await db.commit()
        
        return summary
    
    def _get_total_interactions(self, node: TopicNode) -> int:
        """è®¡ç®—èŠ‚ç‚¹çš„æ€»äº’åŠ¨é‡"""
        if not node.source_item or not node.source_item.interactions:
            return 0
        
        interactions = node.source_item.interactions
        total = 0
        
        for key in ["repost", "comment", "like", "view", "favorite"]:
            if key in interactions:
                total += interactions[key] or 0
        
        return total
    
    def _format_interactions(self, interactions: Optional[Dict]) -> str:
        """æ ¼å¼åŒ–äº’åŠ¨æ•°æ®"""
        if not interactions:
            return ""
        
        parts = []
        if interactions.get("repost"):
            parts.append(f"è½¬å‘{interactions['repost']}")
        if interactions.get("comment"):
            parts.append(f"è¯„è®º{interactions['comment']}")
        if interactions.get("like"):
            parts.append(f"ç‚¹èµ{interactions['like']}")
        
        return ", ".join(parts) if parts else ""
    
    async def _record_judgement(
        self,
        db: AsyncSession,
        topic_id: int,
        type: str,
        prompt: str,
        response: str,
        summary_data: Dict
    ):
        """è®°å½•LLMåˆ¤å®šä»»åŠ¡"""
        # ç¡®ä¿promptæ˜¯å­—ç¬¦ä¸²ç±»å‹
        prompt_str = str(prompt) if not isinstance(prompt, str) else prompt
        
        judgement = LLMJudgement(
            type=type,
            status="completed",
            request_data={
                "topic_id": topic_id,
                "prompt": prompt_str[:1000]  # æˆªæ–­è¿‡é•¿çš„prompt
            },
            response_data=summary_data,
            provider=self.llm_provider.get_provider_name(),
            model=self.llm_provider.model,
            latency_ms=0,  # TODO: è®°å½•å®é™…å»¶è¿Ÿ
            tokens_prompt=0,  # TODO: ä»LLMå“åº”ä¸­è·å–
            tokens_completion=0
        )
        db.add(judgement)
        await db.commit()


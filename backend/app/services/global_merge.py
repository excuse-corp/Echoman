"""
ã€å½’å¹¶é˜¶æ®µäºŒã€‘æ•´ä½“å½’å¹¶æœåŠ¡

âš ï¸ å‘½åè¯´æ˜ï¼š
- æœ¬æ–‡ä»¶åä¸º global_merge.pyï¼ŒåŠŸèƒ½æ˜¯ã€å½’å¹¶é˜¶æ®µäºŒï¼šæ•´ä½“å½’å¹¶ã€‘
- "global" æŒ‡çš„æ˜¯ä¸å†å²Topicå…¨å±€åº“è¿›è¡Œæ¯”å¯¹å½’å¹¶
- æ ¸å¿ƒåŠŸèƒ½ï¼šå†³ç­–æ–°äº‹ä»¶æ˜¯å½’å…¥å·²æœ‰ä¸»é¢˜ï¼Œè¿˜æ˜¯åˆ›å»ºæ–°ä¸»é¢˜

ã€å½’å¹¶æ€»ä½“æµç¨‹ã€‘
æ¯æ—¥æ‰§è¡Œ2æ¬¡å®Œæ•´å½’å¹¶ï¼ˆä¸ŠåŠæ—¥ 12:15-12:30ï¼Œä¸‹åŠæ—¥ 22:15-22:30ï¼‰ï¼š
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ é˜¶æ®µä¸€ï¼šæ–°äº‹ä»¶å½’å¹¶ï¼ˆhalfday_merge.pyï¼Œ12:15/22:15ï¼‰         â”‚
  â”‚ - å¯¹æ–°çˆ¬å–æ•°æ®å»å™ª                                           â”‚
  â”‚ - è¾“å‡ºï¼špending_global_merge                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ é˜¶æ®µäºŒï¼šæ•´ä½“å½’å¹¶ï¼ˆæœ¬æ¨¡å—ï¼Œ12:30/22:30ï¼‰                      â”‚
  â”‚ - ä¸æœ€è¿‘7å¤©Topicæ¯”å¯¹ï¼ˆå‘é‡æ£€ç´¢ + LLMåˆ¤å®šï¼‰                   â”‚
  â”‚ - å†³ç­–ï¼šmergeï¼ˆè¿½åŠ åˆ°å·²æœ‰Topicï¼‰or newï¼ˆåˆ›å»ºæ–°Topicï¼‰       â”‚
  â”‚ - æ›´æ–°çƒ­åº¦ã€åˆ†ç±»ã€æ‘˜è¦                                       â”‚
  â”‚ - è¾“å‡ºï¼šæ›´æ–°Topicsè¡¨ + TopicNodes + å‰ç«¯æ•°æ®æ›´æ–°            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã€æœ¬æ¨¡å—åŠŸèƒ½ã€‘é˜¶æ®µäºŒï¼šæ•´ä½“å½’å¹¶
- è¾“å…¥ï¼šstatus=pending_global_merge ä¸” period åŒ¹é…çš„æ•°æ®
- å¤„ç†ï¼šå‘é‡æ£€ç´¢è¿‘7å¤©Topics â†’ LLMå…³è”åˆ¤å®š â†’ merge or new
- è¾“å‡ºï¼š
  * Topics è¡¨ï¼šæ–°å»ºæˆ–æ›´æ–°ä¸»é¢˜
  * TopicNodes è¡¨ï¼šè®°å½•ä¸»é¢˜èŠ‚ç‚¹
  * TopicPeriodHeat è¡¨ï¼šè®°å½•åŠæ—¥çƒ­åº¦
  * SourceItemsï¼šstatus æ›´æ–°ä¸º merged
  * å‰ç«¯æ•°æ®ï¼šé€šè¿‡ API è½®è¯¢è·å–æœ€æ–° Topics

ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ã€‘
1. æ‰¹é‡å¤„ç†ï¼šæ¯æ¬¡æœ€å¤šå¤„ç† 50 ä¸ªæ–°äº‹ä»¶ç»„
2. å‘é‡æ£€ç´¢é™åˆ¶ï¼šæ¯ä¸ªäº‹ä»¶æœ€å¤šå¬å› Top-10 å€™é€‰ Topics
3. LLM é™æµï¼šæ‰¹é‡åˆ¤å®šæ—¶åˆ†æ‰¹å¤„ç†
4. Token ç®¡ç†ï¼šæ™ºèƒ½æˆªæ–­ï¼Œæ§åˆ¶åœ¨ Qwen3-32B çš„ 32k é™åˆ¶å†…
5. è¶…æ—¶æ§åˆ¶ï¼šå•ä¸ªå½’å¹¶ä»»åŠ¡æœ€é•¿æ‰§è¡Œ 15 åˆ†é’Ÿ
6. èµ„æºç›‘æ§ï¼šè®°å½•æ¯æ¬¡å½’å¹¶çš„è€—æ—¶å’Œèµ„æºä½¿ç”¨
"""
import json
import uuid
import logging
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, date
from app.utils.timezone import now_cn
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_, desc, func
import numpy as np

from app.config import settings
from app.models import (
    SourceItem, Topic, TopicNode, TopicPeriodHeat, 
    LLMJudgement, RunPipeline, Summary
)
from app.services.llm import get_llm_provider, get_embedding_provider
from app.services.classification_service import ClassificationService
from app.services.summary_service import SummaryService
from app.services.vector_service import get_vector_service
from app.utils.token_manager import get_token_manager

logger = logging.getLogger(__name__)


class GlobalMergeService:
    """
    ã€å½’å¹¶é˜¶æ®µäºŒã€‘æ•´ä½“å½’å¹¶æœåŠ¡
    
    âš ï¸ ç±»åè¯´æ˜ï¼šGlobalMergeService è¡¨ç¤ºä¸å…¨å±€Topicåº“è¿›è¡Œå½’å¹¶
    
    èŒè´£ï¼š
    - å°†é˜¶æ®µä¸€è¾“å‡ºçš„éªŒè¯äº‹ä»¶ä¸å†å²ä¸»é¢˜åº“æ¯”å¯¹
    - é€šè¿‡å‘é‡æ£€ç´¢ + LLMåˆ¤å®šï¼Œå†³ç­–æ˜¯å½’å…¥å·²æœ‰ä¸»é¢˜è¿˜æ˜¯åˆ›å»ºæ–°ä¸»é¢˜
    - æ›´æ–° Topicsã€TopicNodesã€TopicPeriodHeat ç­‰è¡¨
    - è§¦å‘å‰ç«¯æ•°æ®æ›´æ–°ï¼ˆé€šè¿‡æ•°æ®åº“æ›´æ–°ï¼Œå‰ç«¯è½®è¯¢APIè·å–ï¼‰
    
    è¾“å…¥ï¼šstatus=pending_global_merge ä¸” period åŒ¹é…çš„æ•°æ®
    è¾“å‡ºï¼š
    - Topics è¡¨ï¼šæ–°å»ºæˆ–æ›´æ–°ä¸»é¢˜
    - TopicNodes è¡¨ï¼šè®°å½•ä¸»é¢˜-æºæ•°æ®å…³è”
    - TopicPeriodHeat è¡¨ï¼šè®°å½•åŠæ—¥çƒ­åº¦å¿«ç…§
    - SourceItemsï¼šstatus æ›´æ–°ä¸º merged
    
    æ€§èƒ½ä¼˜åŒ–ï¼š
    - æ‰¹é‡å¤„ç†ï¼ˆMAX_BATCH_SIZE = 50ï¼‰
    - å‘é‡æ£€ç´¢é™åˆ¶ï¼ˆTOP_K_CANDIDATES = 10ï¼‰
    - LLM æ‰¹é‡åˆ¤å®šåˆ†æ‰¹å¤„ç†
    - Token ç®¡ç†å’Œè¶…æ—¶æ§åˆ¶
    """
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    MAX_BATCH_SIZE = 200  # æ¯æ¬¡æœ€å¤šå¤„ç†200ä¸ªæ–°äº‹ä»¶ç»„ï¼ˆä¼˜åŒ–æå‡4xï¼‰
    MAX_TIMEOUT_SECONDS = 900  # 15åˆ†é’Ÿè¶…æ—¶
    
    def __init__(self, db: AsyncSession):
        """
        åˆå§‹åŒ–æœåŠ¡
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
        """
        self.db = db
        self.llm_provider = get_llm_provider()
        self.embedding_provider = get_embedding_provider()
        self.classification_service = ClassificationService()
        self.summary_service = SummaryService()
        self.token_manager = get_token_manager(model=settings.qwen_model)
        # Token é™åˆ¶ï¼šæ•´ä½“å½’å¹¶ä¸Šä¸‹æ–‡åŒ…å«å€™é€‰ä¸»é¢˜ä¿¡æ¯
        self.max_prompt_tokens = 2500  # è¾“å…¥ä¸Šä¸‹æ–‡æœ€å¤§ token
        self.max_completion_tokens = 300  # åˆ¤å®šç»“æœæœ€å¤§ token
        self.max_candidate_summary_tokens = 200  # æ¯ä¸ªå€™é€‰ä¸»é¢˜æ‘˜è¦æœ€å¤§ token
    
    async def _ensure_summary_vector(self, topic: Topic, representative: SourceItem):
        """
        ç¡®ä¿ Topic å…·å¤‡å¯æ£€ç´¢çš„æ‘˜è¦å‘é‡
        - è‹¥å·²æœ‰æ‘˜è¦ä½†æ— å‘é‡ï¼Œåˆ™é‡å†™å‘é‡
        - è‹¥æ— æ‘˜è¦ï¼Œåˆ™åˆ›å»ºå ä½æ‘˜è¦ï¼ˆç”¨æ ‡é¢˜+æ‘˜è¦ï¼‰ï¼Œå¹¶å†™å‘é‡
        """
        vector_service = get_vector_service()
        try:
            # å·²æœ‰æ‘˜è¦ï¼Œæ£€æŸ¥å‘é‡
            if topic.summary_id:
                vec = vector_service.get_embedding("topic_summary", int(topic.summary_id))
                if vec is not None:
                    return
                # é‡å†™å‘é‡
                stmt = select(Summary).where(Summary.id == topic.summary_id)
                result = await self.db.execute(stmt)
                summary = result.scalar_one_or_none()
                if summary:
                    await self.summary_service._generate_summary_embedding(self.db, summary)  # type: ignore
                    return
            
            # æ²¡æœ‰æ‘˜è¦ï¼Œåˆ›å»ºå ä½æ‘˜è¦ï¼ˆå¸¦å‘é‡ï¼‰
            await self.summary_service._create_placeholder_summary(self.db, topic)  # type: ignore
            
        except Exception as e:
            logger.warning(f\"ç¡®ä¿æ‘˜è¦å‘é‡å¤±è´¥ (Topic {topic.id}): {e}\")
    
    async def run_global_merge(self, period: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•´ä½“å½’å¹¶ï¼ˆé˜¶æ®µäºŒï¼‰
        
        å°†æ–°äº‹ä»¶å½’å¹¶çš„ç»“æœä¸å†å²ä¸»é¢˜åº“æ¯”å¯¹ï¼Œå…³è”æ¼”è¿›æˆ–åˆ›å»ºæ–°ä¸»é¢˜
        
        æ€§èƒ½ä¼˜åŒ–ï¼š
        - æ‰¹é‡å¤„ç†ï¼šæ¯æ¬¡æœ€å¤šå¤„ç† MAX_BATCH_SIZE ä¸ªæ–°äº‹ä»¶ç»„
        - èµ„æºç›‘æ§ï¼šè®°å½•è€—æ—¶å’Œå¤„ç†æ•°é‡
        """
        print(f"ğŸŒ å¼€å§‹æ•´ä½“å½’å¹¶ï¼ˆé˜¶æ®µäºŒï¼‰: {period}")
        start_time = now_cn()
        # åˆ›å»ºè¿è¡Œè®°å½•
        run_id = f"global_merge_{uuid.uuid4().hex[:12]}"
        run_record = RunPipeline(
            run_id=run_id,
            stage="global_merge",
            status="running",
            started_at=start_time
        )
        self.db.add(run_record)
        await self.db.commit()
        try:
            # 1. è·å–åŠæ—¥å½’å¹¶åä¿ç•™çš„äº‹ä»¶
            merge_groups = await self._get_pending_merge_groups(period)
            if not merge_groups:
                run_record.status = "success"
                run_record.ended_at = now_cn()
                run_record.duration_ms = int((run_record.ended_at - start_time).total_seconds() * 1000)
                run_record.input_count = 0
                run_record.output_count = 0
                run_record.success_count = 0
                run_record.results = {
                    "status": "no_data",
                    "period": period,
                    "input_events": 0
                }
                await self.db.commit()
                return {
                    "status": "no_data",
                    "period": period,
                    "input_events": 0
                }
            total_groups = len(merge_groups)
            print(f"ğŸ“Š å¾…å½’å¹¶äº‹ä»¶ç»„: {total_groups} ä¸ª")
            if total_groups > self.MAX_BATCH_SIZE:
                print(
                    f"âš ï¸  äº‹ä»¶ç»„æ•°é‡({total_groups})è¶…è¿‡æ‰¹é‡å¤„ç†é™åˆ¶({self.MAX_BATCH_SIZE})ï¼Œ"
                    f"å°†åªå¤„ç†å‰ {self.MAX_BATCH_SIZE} ä¸ª"
                )
                merge_groups = merge_groups[:self.MAX_BATCH_SIZE]
            merge_count = 0
            new_count = 0
            new_topics = []
            CONCURRENT_BATCH_SIZE = 1  # ä¸²è¡Œï¼Œé¿å…ä¼šè¯å†²çª
            print(f"ğŸš€ å¼€å§‹å¤„ç†ï¼ˆæ¯æ‰¹{CONCURRENT_BATCH_SIZE}ä¸ªï¼‰...")
            for i in range(0, len(merge_groups), CONCURRENT_BATCH_SIZE):
                batch = merge_groups[i:i + CONCURRENT_BATCH_SIZE]
                batch_start = now_cn()
                results = await asyncio.gather(
                    *[self._process_event_group(group, period) for group in batch],
                    return_exceptions=True
                )
                for idx, result in enumerate(results):
                    if isinstance(result, Exception):
                        print(f"  âŒ Group {i + idx} å¤„ç†å¤±è´¥: {result}")
                        continue
                    if result.get("action") == "merge":
                        merge_count += 1
                    elif result.get("action") == "new":
                        new_count += 1
                        if "topic" in result:
                            new_topics.append(result["topic"])
                batch_duration = (now_cn() - batch_start).total_seconds()
                print(f"  âœ… æ‰¹æ¬¡ {i//CONCURRENT_BATCH_SIZE + 1}/{(len(merge_groups)-1)//CONCURRENT_BATCH_SIZE + 1} å®Œæˆ "
                      f"({len(batch)}ä¸ªgroup, è€—æ—¶{batch_duration:.2f}ç§’)")
            if new_topics:
                print(f"\\nğŸ“ å¼€å§‹æ‰¹é‡ç”Ÿæˆæ‘˜è¦ï¼ˆ{len(new_topics)}ä¸ªæ–°Topicï¼‰...")
                await self._batch_generate_summaries(new_topics)
            end_time = now_cn()
            duration_seconds = (end_time - start_time).total_seconds()
            print(f"âœ… å½’å¹¶å®Œæˆ: merge={merge_count}, new={new_count}, è€—æ—¶={duration_seconds:.2f}ç§’")
            merge_stats = {
                "status": "success",
                "period": period,
                "total_groups": total_groups,
                "processed_groups": len(merge_groups),
                "merge_count": merge_count,
                "new_count": new_count,
                "merge_rate": merge_count / len(merge_groups) if merge_groups else 0,
                "duration_seconds": duration_seconds,
                "avg_seconds_per_group": duration_seconds / len(merge_groups) if merge_groups else 0
            }
            try:
                from app.services.frontend_update_service import update_frontend_after_merge
                await update_frontend_after_merge(self.db, period, merge_stats)
            except Exception as e:
                print(f"  âš ï¸  å‰ç«¯æ•°æ®æ›´æ–°å¤±è´¥ï¼ˆä¸å½±å“å½’å¹¶ï¼‰: {e}")
            run_record.status = "success"
            run_record.ended_at = end_time
            run_record.duration_ms = int(duration_seconds * 1000)
            run_record.input_count = total_groups
            run_record.output_count = merge_count + new_count
            run_record.success_count = merge_count + new_count
            run_record.results = merge_stats
            await self.db.commit()
            return merge_stats
        except Exception as e:
            run_record.status = "failed"
            run_record.ended_at = now_cn()
            run_record.duration_ms = int((run_record.ended_at - start_time).total_seconds() * 1000)
            run_record.error_summary = str(e)
            await self.db.commit()
            raise
    async def _get_pending_merge_groups(self, period: str) -> List[Dict[str, Any]]:
        """è·å–å¾…æ•´ä½“å½’å¹¶çš„äº‹ä»¶ç»„"""
        stmt = select(SourceItem).where(
            and_(
                SourceItem.period == period,
                SourceItem.merge_status == "pending_global_merge"
            )
        )
        result = await self.db.execute(stmt)
        items = result.scalars().all()
        
        if not items:
            return []
        
        # æŒ‰å½’å¹¶ç»„åˆ†ç»„
        groups_dict = {}
        for item in items:
            group_id = item.period_merge_group_id
            if group_id not in groups_dict:
                groups_dict[group_id] = []
            groups_dict[group_id].append(item)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨
        groups = [
            {
                "group_id": group_id,
                "items": items_list,
                "representative": items_list[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºä»£è¡¨
            }
            for group_id, items_list in groups_dict.items()
        ]
        
        return groups
    
    async def _process_event_group(
        self,
        event_group: Dict[str, Any],
        period: str
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªäº‹ä»¶ç»„
        
        Returns:
            å†³ç­–ç»“æœ {"action": "merge"|"new", "target_topic_id": ...}
        """
        representative = event_group["representative"]
        items = event_group["items"]
        
        # 1. å‘é‡æ£€ç´¢å€™é€‰ Topics
        candidates = await self._retrieve_candidate_topics(representative)
        
        if not candidates:
            # æ— å€™é€‰ï¼Œç›´æ¥åˆ›å»ºæ–° Topic
            topic = await self._create_new_topic(event_group, period)
            return {"action": "new", "target_topic_id": topic.id, "topic": topic}
        
        # 2. LLM å…³è”åˆ¤å®š
        decision = await self._llm_judge_relation(representative, candidates, period)
        
        if decision["action"] == "merge":
            # å½’å¹¶åˆ°å·²æœ‰ Topic
            # ç¡®ä¿topic_idæ˜¯æ•´æ•°ç±»å‹ï¼ˆLLMå¯èƒ½è¿”å›å­—ç¬¦ä¸²æˆ–å…¶ä»–æ ¼å¼ï¼‰
            try:
                if isinstance(decision["target_topic_id"], str):
                    # å°è¯•ä»å­—ç¬¦ä¸²ä¸­æå–æ•°å­—ï¼ˆå¤„ç†"å€™é€‰ä¸»é¢˜ 1"è¿™æ ·çš„æƒ…å†µï¼‰
                    import re
                    match = re.search(r'\d+', decision["target_topic_id"])
                    if match:
                        topic_id = int(match.group())
                    else:
                        topic_id = int(decision["target_topic_id"])
                else:
                    topic_id = int(decision["target_topic_id"])
                
                await self._merge_to_topic(
                    topic_id,
                    event_group,
                    period
                )
                return {"action": "merge", "target_topic_id": topic_id}
            except (ValueError, TypeError) as e:
                print(f"  âš ï¸  æ— æ³•è§£ætopic_id: {decision.get('target_topic_id')}, é”™è¯¯: {e}")
                print(f"  âš ï¸  æ”¹ä¸ºåˆ›å»ºæ–°Topic")
                # è§£æå¤±è´¥ï¼Œé™çº§ä¸ºåˆ›å»ºæ–°Topic
                topic = await self._create_new_topic(event_group, period)
                return {"action": "new", "target_topic_id": topic.id, "topic": topic}
        else:
            # åˆ›å»ºæ–° Topic
            topic = await self._create_new_topic(event_group, period)
            return {"action": "new", "target_topic_id": topic.id, "topic": topic}
    
    async def _retrieve_candidate_topics(
        self,
        item: SourceItem,
        top_k: int = None
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡æ£€ç´¢å€™é€‰ Topicsï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨Summaryå‘é‡ï¼‰
        
        ã€æ€§èƒ½ä¼˜åŒ–ã€‘
        1. ç›´æ¥æ£€ç´¢topic_summaryå‘é‡ï¼ˆè´¨é‡æ›´é«˜ï¼Œæ•°é‡æ›´å°‘ï¼‰
        2. æ¯ä¸ªäº‹ä»¶æœ€å¤šå¬å› Top-K å€™é€‰ Topicsï¼ˆé»˜è®¤3ä¸ªï¼Œæœ€å¤š3ä¸ªï¼‰
        3. åªä¸æœ€è¿‘7å¤©çš„Topicæ¯”å¯¹ï¼Œé¿å…ä¸è¿‡æ—¶äº‹ä»¶å…³è”
        4. ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ï¼ˆâ‰¥0.5ï¼‰ï¼Œè¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„å€™é€‰
        
        Args:
            item: ä»£è¡¨æ€§æ•°æ®é¡¹
            top_k: è¿”å›æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼Œæœ€å¤š3ä¸ªï¼‰
            
        Returns:
            å€™é€‰ Topics åˆ—è¡¨ï¼ˆç›¸ä¼¼åº¦ç”±é«˜åˆ°ä½ï¼Œæœ€å¤š3ä¸ªï¼‰
        """
        top_k = top_k or settings.global_merge_topk_candidates
        
        # ç¡®ä¿ä¸è¶…è¿‡3ä¸ªå€™é€‰ï¼ˆæ€§èƒ½ä¼˜åŒ–+æˆæœ¬æ§åˆ¶ï¼‰
        top_k = min(top_k, 3)
        
        # è®¡ç®—æ—¶é—´çª—å£ï¼ˆé»˜è®¤åŠå¹´å†…ï¼‰
        from datetime import timedelta
        active_since = now_cn() - timedelta(days=180)
        
        # è·å– item çš„å‘é‡ï¼ˆChromaï¼‰
        vector_service = get_vector_service()
        item_vector = vector_service.get_embedding("source_item", int(item.id))
        if item_vector is None or len(item_vector) == 0:
            return []
        # ç¡®ä¿ä½¿ç”¨ Python listï¼Œé¿å… numpy array å¸ƒå°”åˆ¤æ–­æ­§ä¹‰
        if not isinstance(item_vector, list):
            item_vector = list(item_vector)
        
        # å°è¯•ä½¿ç”¨Chromaè¿›è¡Œå‘é‡æœç´¢
        candidates = []
        
        if vector_service.db_type == "chroma":
            try:
                # ã€ä¼˜åŒ–ã€‘ç›´æ¥æœç´¢topic_summaryå‘é‡ï¼Œè€Œésource_itemå‘é‡
                ids, distances, metadatas = vector_service.search_similar(
                    query_embedding=item_vector,
                    top_k=top_k * 2,  # å¤šå¬å›ä¸€äº›ï¼Œç„¶åæ—¶é—´è¿‡æ»¤
                    where={"object_type": "topic_summary"}  # æœç´¢Summaryå‘é‡
                )
                
                if ids:
                    seen_topics = set()
                    for id_str, distance, metadata in zip(ids, distances, metadatas):
                        similarity = 1 - distance  # è·ç¦»è½¬ç›¸ä¼¼åº¦
                        
                        # ã€ä¼˜åŒ–ã€‘ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                        if similarity < settings.global_merge_similarity_threshold:
                            continue  # è·³è¿‡ç›¸ä¼¼åº¦è¿‡ä½çš„å€™é€‰
                        
                        # ä»metadataç›´æ¥è·å–topic_idï¼ˆæ— éœ€æŸ¥è¯¢TopicNodeï¼‰
                        topic_id = metadata.get("topic_id")
                        if not topic_id or topic_id in seen_topics:
                            continue
                        
                        # æŸ¥è¯¢Topicï¼ˆåªæ£€ç´¢æœ€è¿‘7å¤©çš„æ´»è·ƒTopicï¼‰
                        stmt = select(Topic).where(
                            and_(
                                Topic.id == topic_id,
                                Topic.status == "active",
                                Topic.last_active >= active_since  # æ—¶é—´è¿‡æ»¤
                            )
                        )
                        result = await self.db.execute(stmt)
                        topic = result.scalar_one_or_none()
                        
                        if topic:
                            seen_topics.add(topic.id)
                            candidates.append({
                                "topic_id": topic.id,
                                "title": topic.title_key,
                                "last_active": topic.last_active,
                                "length_hours": (topic.last_active - topic.first_seen).total_seconds() / 3600,
                                "similarity": similarity
                            })
                            
                            if len(candidates) >= top_k:
                                break
                    
                    if candidates:
                        print(f"âœ… ä½¿ç”¨Summaryå‘é‡æ£€ç´¢åˆ° {len(candidates)} ä¸ªå€™é€‰Topicsï¼ˆç›¸ä¼¼åº¦ â‰¥ {settings.global_merge_similarity_threshold}ï¼‰")
                        return candidates
                        
            except Exception as e:
                print(f"âš ï¸  Summaryå‘é‡æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æŸ¥è¯¢: {e}")
        
        # å›é€€æ–¹æ¡ˆï¼šè·å–æœ€è¿‘æ´»è·ƒçš„topicsï¼ˆåªæ£€ç´¢æœ€è¿‘7å¤©çš„ï¼‰
        stmt = select(Topic).where(
            and_(
                Topic.status == "active",
                Topic.last_active >= active_since  # åŠå¹´å†…
            )
        ).order_by(
            Topic.last_active.desc()
        ).limit(top_k)
        result = await self.db.execute(stmt)
        topics = result.scalars().all()
        
        for topic in topics:
            candidates.append({
                "topic_id": topic.id,
                "title": topic.title_key,
                "last_active": topic.last_active,
                "length_hours": (topic.last_active - topic.first_seen).total_seconds() / 3600
            })
        
        return candidates
    
    async def _llm_judge_relation(
        self,
        item: SourceItem,
        candidates: List[Dict[str, Any]],
        period: str
    ) -> Dict[str, Any]:
        """
        LLM åˆ¤å®šæ–°äº‹ä»¶ä¸å€™é€‰ Topics çš„å…³è”æ€§
        
        Returns:
            å†³ç­– {"action": "merge"|"new", "target_topic_id": ..., "confidence": ...}
        """
        if not candidates:
            return {"action": "new", "confidence": 1.0}
        
        # æ„å»º Promptï¼ˆå¸¦æ–‡æœ¬æˆªæ–­ï¼‰
        date_str, period = period.split("_")
        
        # æˆªæ–­æ–°äº‹ä»¶çš„æ ‡é¢˜å’Œæ‘˜è¦
        title = self.token_manager.truncate_text(item.title, max_tokens=80)
        summary = item.summary or 'æ— '
        if summary != 'æ— ':
            summary = self.token_manager.truncate_text(summary, max_tokens=150)
        
        new_event_desc = (
            f"æ ‡é¢˜: {title}\n"
            f"æ‘˜è¦: {summary}\n"
            f"å¹³å°: {item.platform}\n"
            f"æ—¥æœŸ: {date_str} {period}"
        )
        
        # æˆªæ–­å€™é€‰ä¸»é¢˜ä¿¡æ¯
        candidates_desc = []
        for idx, cand in enumerate(candidates, 1):
            cand_title = self.token_manager.truncate_text(
                cand['title'],
                max_tokens=self.max_candidate_summary_tokens  # æ¯ä¸ªå€™é€‰æœ€å¤š 200 tokens
            )
            candidates_desc.append(
                f"ã€å€™é€‰ä¸»é¢˜ {idx}ã€‘\n"
                f"ä¸»é¢˜ID: {cand['topic_id']}\n"
                f"æ ‡é¢˜: {cand_title}\n"
                f"æœ€åæ´»è·ƒ: {cand['last_active'].strftime('%Y-%m-%d %H:%M')}\n"
                f"æŒç»­æ—¶é•¿: {cand['length_hours']:.1f} å°æ—¶"
            )
        
        prompt = f"""åˆ¤æ–­æ–°äº‹ä»¶æ˜¯å¦ä¸ºå·²æœ‰ä¸»é¢˜çš„æ–°è¿›å±•ï¼š

ã€æ–°äº‹ä»¶ã€‘
{new_event_desc}

{chr(10).join(candidates_desc)}

è¦æ±‚è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "decision": "merge" æˆ– "new",
  "target_topic_id": ä¸Šè¿°å€™é€‰ä¸»é¢˜çš„çœŸå®ä¸»é¢˜IDï¼ˆæ•°å­—ï¼‰ï¼Œ
  "confidence": 0.0-1.0,
  "reason": "åˆ¤æ–­ç†ç”±"
}}

åˆ¤æ–­æ ‡å‡†ï¼š
1. å¦‚æœæ–°äº‹ä»¶æ˜¯æŸä¸ªå€™é€‰ä¸»é¢˜çš„åç»­è¿›å±•ã€æ–°æŠ¥é“ï¼Œåˆ™é€‰æ‹© "merge"
2. å¦‚æœæ–°äº‹ä»¶ä¸æ‰€æœ‰å€™é€‰ä¸»é¢˜éƒ½æ— å…³ï¼Œåˆ™é€‰æ‹© "new"
3. æ—¶é—´é—´éš”ä¸è¶…è¿‡7å¤©
4. ä¸»é¢˜ä¸€è‡´æ€§å¼º
"""
        
        # Token ä¼˜åŒ–ï¼šç¡®ä¿ prompt ä¸è¶…è¿‡é™åˆ¶
        prompt_tokens = self.token_manager.count_tokens(prompt)
        if prompt_tokens > self.max_prompt_tokens:
            logger.warning(
                f"æ•´ä½“å½’å¹¶ prompt è¿‡é•¿ ({prompt_tokens} tokens)ï¼Œéœ€è¦æˆªæ–­"
            )
            prompt = self.token_manager.truncate_text(
                prompt,
                max_tokens=self.max_prompt_tokens
            )
            logger.info(f"æˆªæ–­å prompt: {self.token_manager.count_tokens(prompt)} tokens")
        
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ–°é—»äº‹ä»¶åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿åˆ¤æ–­äº‹ä»¶ä¹‹é—´çš„å…³è”æ€§ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            response = await self.llm_provider.chat_completion(
                messages,
                response_format="json",
                max_tokens=self.max_completion_tokens  # ä½¿ç”¨é…ç½®çš„å€¼ï¼ˆ300ï¼‰
            )
            
            result = json.loads(response["content"])
            
            # è®°å½• Token ä½¿ç”¨
            logger.info(
                f"æ•´ä½“å½’å¹¶åˆ¤å®šå®Œæˆ - Prompt: {prompt_tokens} tokens, "
                f"Completion: {response.get('usage', {}).get('completion_tokens', 0)} tokens, "
                f"å†³ç­–: {result.get('decision')} (ç½®ä¿¡åº¦: {result.get('confidence')})"
            )
            resolved_topic_id = self._resolve_llm_target_topic_id(
                result.get("target_topic_id"),
                candidates
            )
            result["resolved_topic_id"] = resolved_topic_id
            
            # è®°å½•åˆ¤å®š
            judgement = LLMJudgement(
                type="global_merge",
                status="success",
                request={
                    "item_id": item.id,
                    "candidates": [c["topic_id"] for c in candidates]
                },
                response=result,
                tokens_prompt=response["usage"].get("prompt_tokens"),
                tokens_completion=response["usage"].get("completion_tokens"),
                provider=self.llm_provider.get_provider_name(),
                model=self.llm_provider.model
            )
            self.db.add(judgement)
            # å…ˆflushç¡®ä¿IDç«‹å³åˆ†é…ï¼Œé¿å…å¹¶è¡Œå†²çª
            await self.db.flush()
            await self.db.commit()
            
            # æ£€æŸ¥ç½®ä¿¡åº¦
            if (
                result.get("decision") == "merge" 
                and result.get("confidence", 0) >= settings.global_merge_confidence_threshold
                and resolved_topic_id is not None
            ):
                return {
                    "action": "merge",
                    "target_topic_id": resolved_topic_id,
                    "confidence": result.get("confidence"),
                    "reason": result.get("reason")
                }
            else:
                return {
                    "action": "new",
                    "confidence": result.get("confidence", 0),
                    "reason": result.get("reason")
                }
            
        except Exception as e:
            print(f"âŒ LLM åˆ¤å®šå¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä¿å®ˆå¤„ç†ï¼šåˆ›å»ºæ–° Topic
            return {"action": "new", "confidence": 0.5}

    def _resolve_llm_target_topic_id(
        self,
        raw_target: Any,
        candidates: List[Dict[str, Any]]
    ) -> Optional[int]:
        """å°†LLMè¿”å›çš„target_topic_idè§£æä¸ºçœŸå®Topic ID"""
        if raw_target is None:
            return None
        
        candidate_ids = [cand["topic_id"] for cand in candidates]
        
        # å¦‚æœç›´æ¥æ˜¯æ•´æ•°ä¸”å­˜åœ¨äºå€™é€‰IDåˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(raw_target, int):
            if raw_target in candidate_ids:
                return raw_target
            # å…¼å®¹åªè¿”å›åºå·çš„æƒ…å†µ
            if 1 <= raw_target <= len(candidate_ids):
                return candidate_ids[raw_target - 1]
            return None
        
        # å¦‚æœæ˜¯æµ®ç‚¹æ•°ï¼ˆLLMå¯èƒ½è¿”å›1.0ï¼‰
        if isinstance(raw_target, float):
            raw_int = int(raw_target)
            if raw_int in candidate_ids:
                return raw_int
            if 1 <= raw_int <= len(candidate_ids):
                return candidate_ids[raw_int - 1]
            return None
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£ææ•°å­—
        if isinstance(raw_target, str):
            raw_target = raw_target.strip()
            import re
            match = re.search(r'\d+', raw_target)
            if not match:
                return None
            value = int(match.group())
            if value in candidate_ids:
                return value
            if 1 <= value <= len(candidate_ids):
                return candidate_ids[value - 1]
            return None
        
        return None
    
    async def _create_new_topic(
        self,
        event_group: Dict[str, Any],
        period: str
    ) -> Topic:
        """åˆ›å»ºæ–° Topic"""
        items = event_group["items"]
        representative = event_group["representative"]
        
        try:
            # åˆ›å»º Topic
            topic = Topic(
                title_key=representative.title,
                first_seen=min(item.fetched_at for item in items),
                last_active=max(item.fetched_at for item in items),
                status="active",
                intensity_total=len(items),
                current_heat_normalized=sum(
                    item.heat_normalized or 0 for item in items
                ) / len(items) if items else 0
            )
            self.db.add(topic)
            await self.db.flush()  # è·å– topic.id
            
            # åˆ›å»º TopicNodes
            nodes_created = 0
            for item in items:
                node = TopicNode(
                    topic_id=topic.id,
                    source_item_id=item.id,
                    appended_at=now_cn()
                )
                self.db.add(node)
                nodes_created += 1
                
                # æ›´æ–° item çŠ¶æ€
                item.merge_status = "merged"
            
            # æ›´æ–°åŠæ—¥çƒ­åº¦è®°å½•
            await self._update_topic_heat(topic, event_group, period)
            
            # æäº¤å‰flushï¼Œç¡®ä¿nodesè¢«ä¿å­˜
            await self.db.flush()
            
            # æœ€ç»ˆæäº¤
            await self.db.commit()
            
            print(f"  âœ¨ åˆ›å»ºæ–° Topic: {topic.id} - {topic.title_key} ({nodes_created} nodes)")

            # ç«‹å³å†™å ä½æ‘˜è¦å‘é‡ï¼Œé¿å…åŒè½®é‡å¤åˆ›å»º
            await self._ensure_summary_vector(topic, representative)
            
            # ã€æ€§èƒ½ä¼˜åŒ–ã€‘åˆ†ç±»å’Œæ‘˜è¦ç”Ÿæˆå»¶è¿Ÿåˆ°æ‰¹é‡å¤„ç†
            # å¼‚æ­¥æ‰§è¡Œåˆ†ç±»ï¼ˆä¸ç­‰å¾…æ‘˜è¦ç”Ÿæˆï¼‰
            try:
                # åˆ·æ–°ä¼šè¯ä»¥ç¡®ä¿èƒ½æŸ¥è¯¢åˆ°åˆšåˆ›å»ºçš„nodes
                await self.db.refresh(topic)
                
                category, confidence, method = await self.classification_service.classify_topic(
                    self.db, topic, force_llm=False
                )
                topic.category = category
                topic.category_confidence = confidence
                topic.category_method = method
                topic.category_updated_at = now_cn()
                await self.db.commit()
                print(f"  ğŸ“‹ å®Œæˆåˆ†ç±»: {category} (ç½®ä¿¡åº¦: {confidence:.2f}, æ–¹æ³•: {method})")
            except Exception as e:
                logger.error(f"åˆ†ç±»å¤±è´¥: {e}")
                print(f"  âŒ åˆ†ç±»å¤±è´¥: {e}")
                # åˆ†ç±»å¤±è´¥ä¸å½±å“Topicåˆ›å»º
                await self.db.rollback()
                await self.db.commit()  # é‡æ–°æäº¤topicåˆ›å»º
            
            # æ‘˜è¦ç”Ÿæˆå°†åœ¨æ‰¹é‡å¤„ç†ä¸­å®Œæˆ
            return topic
            
        except Exception as e:
            logger.error(f"åˆ›å»ºTopicå¤±è´¥: {e}")
            print(f"  âŒ åˆ›å»ºTopicå¤±è´¥: {e}")
            await self.db.rollback()
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
    
    async def _merge_to_topic(
        self,
        topic_id: int,
        event_group: Dict[str, Any],
        period: str
    ):
        """å½’å¹¶åˆ°å·²æœ‰ Topic"""
        items = event_group["items"]
        
        # è·å– Topic
        stmt = select(Topic).where(Topic.id == topic_id)
        result = await self.db.execute(stmt)
        topic = result.scalar_one_or_none()
        
        if not topic:
            print(f"  âŒ Topic {topic_id} ä¸å­˜åœ¨")
            return
        
        # æ›´æ–° Topic
        topic.last_active = max(item.fetched_at for item in items)
        topic.intensity_total += len(items)
        
        # è¿½åŠ  TopicNodes
        for item in items:
            node = TopicNode(
                topic_id=topic.id,
                source_item_id=item.id,
                appended_at=now_cn()
            )
            self.db.add(node)
            
            item.merge_status = "merged"
        
        # æ›´æ–°åŠæ—¥çƒ­åº¦
        await self._update_topic_heat(topic, event_group, period)
        
        # è·å–æ–°å¢çš„èŠ‚ç‚¹ç”¨äºå¢é‡æ‘˜è¦
        new_nodes = []
        for item in items:
            stmt = select(TopicNode).where(
                and_(
                    TopicNode.topic_id == topic.id,
                    TopicNode.source_item_id == item.id
                )
            )
            result = await self.db.execute(stmt)
            node = result.scalar_one_or_none()
            if node:
                new_nodes.append(node)
        
        await self.db.commit()
        
        print(f"  ğŸ”— å½’å¹¶åˆ° Topic: {topic.id} - {topic.title_key}")

        # è‹¥æ—§Topicç¼ºæ‘˜è¦å‘é‡ï¼Œç«‹å³è¡¥å†™å ä½æ‘˜è¦å‘é‡
        await self._ensure_summary_vector(topic, items[0])
        
        # å¼‚æ­¥æ‰§è¡Œå¢é‡æ‘˜è¦æ›´æ–°
        try:
            print(f"  ğŸ“ å¼€å§‹å¢é‡æ‘˜è¦æ›´æ–°... (æ–°èŠ‚ç‚¹æ•°: {len(new_nodes)})")
            updated_summary = await self.summary_service.generate_or_update_summary(
                self.db, topic, new_nodes
            )
            if updated_summary:
                print(f"  âœ… æ‘˜è¦æ›´æ–°å®Œæˆ (æ–¹æ³•: {updated_summary.method})")
            else:
                print(f"  â„¹ï¸  æ— éœ€æ›´æ–°æ‘˜è¦")
            
        except Exception as e:
            logger.error(f"æ‘˜è¦æ›´æ–°å¤±è´¥: {e}")
            import traceback
            logger.error(f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
            print(f"  âŒ æ‘˜è¦æ›´æ–°å¤±è´¥: {e}")
    
    async def _update_topic_heat(
        self,
        topic: Topic,
        event_group: Dict[str, Any],
        period: str
    ):
        """æ›´æ–° Topic çš„åŠæ—¥çƒ­åº¦è®°å½•"""
        items = event_group["items"]
        
        # è§£æåŠæ—¥æ—¶æ®µ
        date_str, period = period.split("_")
        date_obj = datetime.strptime(date_str, "%Y-%m-%d").date()
        
        # è®¡ç®—åŠæ—¥çƒ­åº¦
        heat_normalized = sum(
            item.heat_normalized or 0 for item in items
        ) / len(items) if items else 0
        
        # æŸ¥æ‰¾æˆ–åˆ›å»ºåŠæ—¥çƒ­åº¦è®°å½•
        stmt = select(TopicPeriodHeat).where(
            and_(
                TopicPeriodHeat.topic_id == topic.id,
                TopicPeriodHeat.date == date_obj,
                TopicPeriodHeat.period == period
            )
        )
        result = await self.db.execute(stmt)
        heat_record = result.scalar_one_or_none()
        
        if heat_record:
            # æ›´æ–°ç°æœ‰è®°å½•
            heat_record.heat_normalized = heat_normalized
            heat_record.heat_percentage = heat_normalized * 100
            heat_record.source_count += len(items)
        else:
            # åˆ›å»ºæ–°è®°å½•
            heat_record = TopicPeriodHeat(
                topic_id=topic.id,
                date=date_obj,
                period=period,
                heat_normalized=heat_normalized,
                heat_percentage=heat_normalized * 100,
                source_count=len(items)
            )
            self.db.add(heat_record)
        
        # æ›´æ–° Topic çš„å½“å‰çƒ­åº¦
        topic.current_heat_normalized = heat_normalized
        topic.heat_percentage = heat_normalized * 100
    
    async def _batch_generate_summaries(self, topics: List[Topic]):
        """
        æ‰¹é‡å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
        
        Args:
            topics: å¾…ç”Ÿæˆæ‘˜è¦çš„Topicåˆ—è¡¨
        """
        if not topics:
            return
        
        summary_start = now_cn()
        success_count = 0
        failed_count = 0
        
        # å¹¶è¡Œç”Ÿæˆæ‘˜è¦ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
        SUMMARY_CONCURRENT_SIZE = 5  # æ‘˜è¦ç”Ÿæˆå¹¶å‘æ•°è¾ƒå°‘ï¼Œé¿å…LLMé™æµ
        
        for i in range(0, len(topics), SUMMARY_CONCURRENT_SIZE):
            batch = topics[i:i + SUMMARY_CONCURRENT_SIZE]
            
            # å¹¶è¡Œç”Ÿæˆå½“å‰æ‰¹æ¬¡çš„æ‘˜è¦
            results = await asyncio.gather(
                *[self._generate_single_summary(topic) for topic in batch],
                return_exceptions=True
            )
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    failed_count += 1
                elif result:
                    success_count += 1
                else:
                    failed_count += 1
        
        summary_duration = (now_cn() - summary_start).total_seconds()
        print(f"âœ… æ‘˜è¦æ‰¹é‡ç”Ÿæˆå®Œæˆ: æˆåŠŸ{success_count}, å¤±è´¥{failed_count}, "
              f"è€—æ—¶{summary_duration:.2f}ç§’ (å¹³å‡{summary_duration/len(topics):.2f}ç§’/ä¸ª)")
    
    async def _generate_single_summary(self, topic: Topic) -> bool:
        """
        ä¸ºå•ä¸ªTopicç”Ÿæˆæ‘˜è¦ï¼ˆä½¿ç”¨ç‹¬ç«‹æ•°æ®åº“ä¼šè¯é¿å…å¹¶å‘å†²çªï¼‰
        
        Returns:
            True if successful, False otherwise
        """
        from app.core.database import get_async_session
        
        topic_id = topic.id
        
        try:
            print(f"  ğŸ“ å¼€å§‹ç”Ÿæˆæ‘˜è¦... (Topic {topic_id})")
            
            # ä¸ºæ¯ä¸ªæ‘˜è¦ç”Ÿæˆä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯ï¼Œé¿å…å¹¶å‘å†²çª
            async_session_factory = get_async_session()
            async with async_session_factory() as independent_db:
                # é‡æ–°æŸ¥è¯¢ topicï¼ˆåœ¨ç‹¬ç«‹ä¼šè¯ä¸­ï¼‰
                stmt = select(Topic).where(Topic.id == topic_id)
                result = await independent_db.execute(stmt)
                topic_in_session = result.scalar_one_or_none()
                
                if not topic_in_session:
                    print(f"  âŒ Topic {topic_id} ä¸å­˜åœ¨")
                    return False
                
                # ä½¿ç”¨ç‹¬ç«‹ä¼šè¯ç”Ÿæˆæ‘˜è¦
                summary = await self.summary_service.generate_full_summary(
                    independent_db, 
                    topic_in_session
                )
                
                if summary and summary.method == "full":
                    print(f"  âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ (Topic {topic_id}, æ–¹æ³•: {summary.method})")
                    return True
                elif summary and summary.method == "placeholder":
                    print(f"  âš ï¸  åˆ›å»ºäº†å ä½æ‘˜è¦ (Topic {topic_id})")
                    return False
                else:
                    print(f"  âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥ (Topic {topic_id})")
                    return False
                
        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ (Topic {topic_id}): {e}")
            import traceback
            logger.error(f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
            print(f"  âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥ (Topic {topic_id}): {e}")
            return False

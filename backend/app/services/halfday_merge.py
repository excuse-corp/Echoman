"""
ã€å½’å¹¶é˜¶æ®µä¸€ã€‘æ–°äº‹ä»¶å½’å¹¶æœåŠ¡

âš ï¸ å‘½åè¯´æ˜ï¼š
- æœ¬æ–‡ä»¶åä¸º halfday_merge.pyï¼Œä½†å®é™…åŠŸèƒ½æ˜¯ã€å½’å¹¶é˜¶æ®µä¸€ï¼šæ–°äº‹ä»¶å½’å¹¶ã€‘
- "halfday" æ˜¯å†å²å‘½åï¼ŒæŒ‡çš„æ˜¯å¯¹åŠæ—¥å‘¨æœŸå†…é‡‡é›†çš„æ•°æ®è¿›è¡Œå½’å¹¶
- æ ¸å¿ƒåŠŸèƒ½ï¼šå¯¹æ–°é‡‡é›†çš„æ•°æ®å»å™ªã€éªŒè¯çœŸå®çƒ­ç‚¹

ã€å½’å¹¶æ€»ä½“æµç¨‹ã€‘
æ¯æ—¥æ‰§è¡Œ3æ¬¡å®Œæ•´å½’å¹¶ï¼ˆä¸Šåˆ 12:15-12:30ï¼Œä¸‹åˆ 18:15-18:30ï¼Œå‚æ™š 22:15-22:30ï¼‰ï¼š
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ é˜¶æ®µä¸€ï¼šæ–°äº‹ä»¶å½’å¹¶ï¼ˆæœ¬æ¨¡å—ï¼Œ12:15/18:15/22:15ï¼‰             â”‚
  â”‚ - å¯¹æ–°çˆ¬å–æ•°æ®å»å™ª                                           â”‚
  â”‚ - çƒ­åº¦å½’ä¸€åŒ–ï¼ˆMin-Max + å¹³å°æƒé‡ï¼‰                          â”‚
  â”‚ - å‘é‡èšç±»ï¼ˆç›¸ä¼¼åº¦ > 0.85ï¼‰                                 â”‚
  â”‚ - LLMåˆ¤å®šï¼ˆç¡®è®¤åŒç»„äº‹ä»¶ï¼‰                                   â”‚
  â”‚ - å‡ºç°æ¬¡æ•°ç­›é€‰ï¼ˆâ‰¥2æ¬¡ä¿ç•™ï¼Œè¿‡æ»¤å•æ¬¡å™ªéŸ³ï¼‰                    â”‚
  â”‚ - è¾“å‡ºï¼špending_global_merge                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ é˜¶æ®µäºŒï¼šæ•´ä½“å½’å¹¶ï¼ˆglobal_merge.pyï¼Œ12:30/18:30/22:30ï¼‰      â”‚
  â”‚ - ä¸å†å²Topicåº“æ¯”å¯¹                                          â”‚
  â”‚ - å†³ç­–ï¼šå½’å…¥å·²æœ‰ä¸»é¢˜ or åˆ›å»ºæ–°ä¸»é¢˜                          â”‚
  â”‚ - è¾“å‡ºï¼šæ›´æ–°Topicsè¡¨ + å‰ç«¯æ•°æ®æ›´æ–°                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ã€æœ¬æ¨¡å—åŠŸèƒ½ã€‘é˜¶æ®µä¸€ï¼šæ–°äº‹ä»¶å½’å¹¶
- è¾“å…¥ï¼šperiod å†…çš„ pending_event_merge æ•°æ®
- å¤„ç†ï¼šå»å™ª + éªŒè¯çœŸå®æ€§
- è¾“å‡ºï¼šä¿ç•™çš„çœŸå®çƒ­ç‚¹äº‹ä»¶ â†’ pending_global_merge

ä¼˜åŒ–ï¼šæ·»åŠ ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ä»¥å¤„ç† qwen3-32b çš„ 32k ä¸Šä¸‹æ–‡é™åˆ¶
"""
import json
import uuid
import logging
from typing import List, Dict, Any, Tuple
from datetime import datetime
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_, func
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from app.config import settings
from app.models import SourceItem, Embedding, LLMJudgement, RunPipeline
from app.services.llm import get_llm_provider, get_embedding_provider
from app.services.vector_service import get_vector_service
from app.utils.token_manager import get_token_manager
from app.utils.timezone import now_cn

logger = logging.getLogger(__name__)


class EventMergeService:
    """
    ã€å½’å¹¶é˜¶æ®µä¸€ã€‘æ–°äº‹ä»¶å½’å¹¶æœåŠ¡
    
    èŒè´£ï¼š
    - å¯¹å½’å¹¶å‘¨æœŸï¼ˆAM/PM/EVEï¼‰å†…æ–°é‡‡é›†çš„æ•°æ®å»å™ªã€éªŒè¯çœŸå®çƒ­ç‚¹
    - è¿‡æ»¤å•æ¬¡å‡ºç°çš„å™ªéŸ³æ•°æ®ï¼ˆå‡ºç°æ¬¡æ•° < 2ï¼‰
    - ä¿ç•™ç»è¿‡éªŒè¯çš„çœŸå®çƒ­ç‚¹äº‹ä»¶
    
    è¾“å…¥ï¼šstatus=pending_event_merge ä¸” period åŒ¹é…çš„æ•°æ®
    è¾“å‡ºï¼šstatus=pending_global_mergeï¼ˆè¿›å…¥é˜¶æ®µäºŒï¼‰æˆ– status=discardedï¼ˆå™ªéŸ³æ•°æ®ï¼‰
    """
    
    def __init__(self, db: AsyncSession):
        """
        åˆå§‹åŒ–æœåŠ¡
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
        """
        self.db = db
        self.llm_provider = get_llm_provider()
        self.embedding_provider = get_embedding_provider()
        self.token_manager = get_token_manager(model=settings.qwen_model)
        # Token é™åˆ¶ï¼šå½’å¹¶ä»»åŠ¡ä¸Šä¸‹æ–‡é€šå¸¸åŒ…å«å¤šä¸ªæ–°é—»æ¡ç›®
        self.max_prompt_tokens = 2000  # è¾“å…¥ä¸Šä¸‹æ–‡æœ€å¤§ token
        self.max_completion_tokens = 300  # åˆ¤å®šç»“æœæœ€å¤§ token
        self.max_item_summary_tokens = 150  # æ¯ä¸ªæ–°é—»æ‘˜è¦æœ€å¤§ token
    
    async def run_event_merge(self, period: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–°äº‹ä»¶å½’å¹¶
        
        Args:
            period: å½’å¹¶å‘¨æœŸæ ‡è¯†ï¼ˆå¦‚ "2025-10-29_AM"ã€"2025-10-29_PM"ã€"2025-10-29_EVE"ï¼‰
            
        Returns:
            å½’å¹¶ç»“æœç»Ÿè®¡
        """
        print(f"ğŸ”„ å¼€å§‹æ–°äº‹ä»¶å½’å¹¶: {period}")
        
        # åˆ›å»ºè¿è¡Œè®°å½•
        run_id = f"event_merge_{uuid.uuid4().hex[:12]}"
        started_at = now_cn()
        run_record = RunPipeline(
            run_id=run_id,
            stage="event_merge",
            status="running",
            started_at=started_at
        )
        self.db.add(run_record)
        await self.db.commit()
        
        try:
            # 1. è·å–å¾…å½’å¹¶çš„æ•°æ®
            items = await self._get_pending_items(period)
            if not items:
                run_record.status = "success"
                run_record.ended_at = now_cn()
                run_record.duration_ms = int((run_record.ended_at - started_at).total_seconds() * 1000)
                run_record.input_count = 0
                run_record.output_count = 0
                run_record.success_count = 0
                run_record.results = {
                    "status": "no_data",
                    "period": period,
                    "input_items": 0
                }
                await self.db.commit()
                return {
                    "status": "no_data",
                    "period": period,
                    "input_items": 0
                }
            
            print(f"ğŸ“Š å¾…å½’å¹¶æ•°æ®: {len(items)} æ¡")
            
            # 2. å‘é‡åŒ–
            print("ğŸ”¤ å¼€å§‹å‘é‡åŒ–...")
            await self._vectorize_items(items)
            
            # 3. å‘é‡èšç±»
            print("ğŸ”— å¼€å§‹å‘é‡èšç±»...")
            candidate_groups = await self._vector_clustering(items)
            print(f"ğŸ“¦ åˆæ­¥èšç±»: {len(candidate_groups)} ä¸ªå€™é€‰ç»„")
            
            # 4. LLM ç²¾ç¡®åˆ¤å®š
            print("ğŸ¤– å¼€å§‹ LLM åˆ¤å®š...")
            merge_groups = await self._llm_judge_merge(candidate_groups, period)
            print(f"âœ… LLM åˆ¤å®šå®Œæˆ: {len(merge_groups)} ä¸ªå½’å¹¶ç»„")
            
            # 5. å‡ºç°æ¬¡æ•°ç»Ÿè®¡ä¸ç­›é€‰
            print("ğŸ” ç»Ÿè®¡å‡ºç°æ¬¡æ•°å¹¶ç­›é€‰...")
            kept_items, dropped_items = await self._filter_by_occurrence(
                merge_groups,
                min_occurrence=settings.halfday_merge_min_occurrence
            )
            print(f"âœ… ä¿ç•™ {len(kept_items)} æ¡ï¼Œä¸¢å¼ƒ {len(dropped_items)} æ¡")
            
            # 6. çƒ­åº¦èšåˆ
            await self._aggregate_heat(merge_groups)
            
            # 7. å‡†å¤‡è¿”å›ç»“æœ
            result = {
                "status": "success",
                "period": period,
                "input_items": len(items),
                "kept_items": len(kept_items),
                "dropped_items": len(dropped_items),
                "keep_rate": len(kept_items) / len(items) if items else 0,
                "drop_rate": len(dropped_items) / len(items) if items else 0,
                "merge_groups": len(merge_groups),
                "avg_occurrence": sum(
                    len(group['items']) for group in merge_groups
                ) / len(merge_groups) if merge_groups else 0
            }
            
            # æ›´æ–°è¿è¡Œè®°å½•
            run_record.status = "success"
            run_record.ended_at = now_cn()
            run_record.duration_ms = int((run_record.ended_at - started_at).total_seconds() * 1000)
            run_record.input_count = len(items)
            run_record.output_count = len(kept_items)
            run_record.success_count = len(kept_items)
            run_record.failed_count = len(dropped_items)
            run_record.results = result
            await self.db.commit()
            return result
        
        except Exception as e:
            # æ›´æ–°è¿è¡Œè®°å½•ä¸ºå¤±è´¥çŠ¶æ€
            run_record.status = "failed"
            run_record.ended_at = now_cn()
            run_record.duration_ms = int((run_record.ended_at - started_at).total_seconds() * 1000)
            run_record.error_summary = str(e)
            await self.db.commit()
            raise
    
    async def _get_pending_items(self, period: str) -> List[SourceItem]:
        """è·å–å¾…å½’å¹¶çš„æ•°æ®"""
        stmt = select(SourceItem).where(
            and_(
                SourceItem.period == period,
                SourceItem.merge_status == "pending_event_merge"
            )
        )
        result = await self.db.execute(stmt)
        return result.scalars().all()
    
    async def _vectorize_items(self, items: List[SourceItem]):
        """å‘é‡åŒ–æ•°æ®é¡¹"""
        # å‡†å¤‡æ–‡æœ¬
        texts = [
            f"{item.title} {item.summary or ''}" 
            for item in items
        ]
        
        try:
            # æ‰¹é‡å‘é‡åŒ–
            vectors = await self.embedding_provider.embedding(texts)
            
            # ä¿å­˜å‘é‡åˆ°PostgreSQL
            embeddings_to_create = []
            for item, vector in zip(items, vectors):
                embedding = Embedding(
                    object_type="source_item",
                    object_id=item.id,
                    provider=self.embedding_provider.get_provider_name(),
                    model=self.embedding_provider.model,
                    vector=vector
                )
                self.db.add(embedding)
                embeddings_to_create.append((item, embedding))
            
            # å…ˆæäº¤ä»¥è·å–embeddingçš„ID
            await self.db.flush()
            
            # æ›´æ–° source_item çš„ embedding_id
            for item, embedding in embeddings_to_create:
                item.embedding_id = embedding.id
            
            await self.db.commit()
            
            # åŒæ­¥ä¿å­˜åˆ°Chromaå‘é‡æ•°æ®åº“
            try:
                vector_service = get_vector_service()
                if vector_service.db_type == "chroma":
                    ids = [f"source_item_{item.id}" for item in items]
                    metadatas = [
                        {
                            "object_type": "source_item",
                            "object_id": int(item.id),
                            "platform": item.platform,
                            "title": item.title[:200]  # é™åˆ¶é•¿åº¦
                        }
                        for item in items
                    ]
                    documents = [f"{item.title} {item.summary or ''}"[:500] for item in items]
                    
                    vector_service.add_embeddings(
                        ids=ids,
                        embeddings=vectors,
                        metadatas=metadatas,
                        documents=documents
                    )
                    print(f"âœ… å·²åŒæ­¥ {len(vectors)} ä¸ªå‘é‡åˆ°Chroma")
            except Exception as chroma_error:
                print(f"âš ï¸  ChromaåŒæ­¥å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {chroma_error}")
            
        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨æ¨¡æ‹Ÿå‘é‡
            for item in items:
                # ä½¿ç”¨éšæœºå‘é‡ä»£æ›¿ï¼ˆä»…ç”¨äºå¼€å‘æµ‹è¯•ï¼‰
                mock_vector = np.random.rand(settings.embedding_dimension).tolist()
                embedding = Embedding(
                    object_type="source_item",
                    object_id=item.id,
                    provider="mock",
                    model="mock",
                    vector=mock_vector
                )
                self.db.add(embedding)
            
            await self.db.commit()
    
    async def _vector_clustering(
        self,
        items: List[SourceItem]
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡èšç±»
        
        Returns:
            å€™é€‰å½’å¹¶ç»„åˆ—è¡¨
        """
        # è·å–å‘é‡
        item_vectors = []
        for item in items:
            stmt = select(Embedding).where(
                and_(
                    Embedding.object_type == "source_item",
                    Embedding.object_id == item.id
                )
            ).order_by(Embedding.created_at.desc()).limit(1)
            result = await self.db.execute(stmt)
            embedding = result.scalar_one_or_none()
            
            if embedding:
                item_vectors.append((item, embedding.vector))
        
        if not item_vectors:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        vectors = np.array([vec for _, vec in item_vectors])
        similarity_matrix = cosine_similarity(vectors)
        
        # ç®€å•çš„è´ªå¿ƒèšç±»
        threshold = settings.halfday_merge_vector_threshold
        used = set()
        groups = []
        
        for i, (item_i, _) in enumerate(item_vectors):
            if i in used:
                continue
            
            group_items = [item_i]
            group_indices = [i]
            used.add(i)
            
            for j, (item_j, _) in enumerate(item_vectors):
                if j in used or j == i:
                    continue
                
                # æ£€æŸ¥ç›¸ä¼¼åº¦
                if similarity_matrix[i][j] >= threshold:
                    # é¢å¤–æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦
                    title_sim = self._title_jaccard(item_i.title, item_j.title)
                    if title_sim >= settings.halfday_merge_title_threshold:
                        group_items.append(item_j)
                        group_indices.append(j)
                        used.add(j)
            
            if len(group_items) > 0:
                groups.append({
                    "items": group_items,
                    "indices": group_indices
                })
        
        return groups
    
    def _title_jaccard(self, title1: str, title2: str) -> float:
        """è®¡ç®—æ ‡é¢˜ Jaccard ç›¸ä¼¼åº¦ï¼ˆn-gramï¼‰"""
        n = 2  # 2-gram
        
        def get_ngrams(text: str, n: int) -> set:
            return set(text[i:i+n] for i in range(len(text) - n + 1))
        
        ngrams1 = get_ngrams(title1, n)
        ngrams2 = get_ngrams(title2, n)
        
        if not ngrams1 or not ngrams2:
            return 0.0
        
        intersection = len(ngrams1 & ngrams2)
        union = len(ngrams1 | ngrams2)
        
        return intersection / union if union > 0 else 0.0
    
    async def _llm_judge_merge(
        self,
        candidate_groups: List[Dict[str, Any]],
        period: str
    ) -> List[Dict[str, Any]]:
        """
        LLM åˆ¤å®šæ˜¯å¦ä¸ºåŒä¸€äº‹ä»¶
        
        Returns:
            æœ€ç»ˆå½’å¹¶ç»„åˆ—è¡¨
        """
        merge_groups = []
        
        for group in candidate_groups:
            items = group["items"]
            
            if len(items) == 1:
                # å•ä¸ªé¡¹ï¼Œç›´æ¥ä¿ç•™
                group_id = f"halfday_{uuid.uuid4().hex[:8]}"
                merge_groups.append({
                    "group_id": group_id,
                    "items": items,
                    "is_same_event": True,
                    "confidence": 1.0
                })
                continue
            
            # æ„å»º Promptï¼ˆå¸¦æ–‡æœ¬æˆªæ–­ï¼‰
            items_desc = []
            for idx, item in enumerate(items, 1):
                # æˆªæ–­æ ‡é¢˜å’Œæ‘˜è¦ï¼Œé˜²æ­¢è¿‡é•¿
                title = self.token_manager.truncate_text(
                    item.title,
                    max_tokens=80  # æ¯ä¸ªæ ‡é¢˜æœ€å¤š 80 tokens
                )
                summary = item.summary or 'æ— '
                if summary != 'æ— ':
                    summary = self.token_manager.truncate_text(
                        summary,
                        max_tokens=self.max_item_summary_tokens  # æ¯ä¸ªæ‘˜è¦æœ€å¤š 150 tokens
                    )
                
                items_desc.append(
                    f"[Item {idx}] æ ‡é¢˜: {title}  "
                    f"æ‘˜è¦: {summary}  "
                    f"å¹³å°: {item.platform}  "
                    f"æ—¶é—´: {item.fetched_at.strftime('%H:%M')}"
                )
            
            prompt = f"""åˆ¤æ–­ä»¥ä¸‹æ–°é—»æ¡ç›®æ˜¯å¦ä¸ºåŒä¸€äº‹ä»¶çš„ä¸åŒæŠ¥é“ï¼ˆåŠæ—¥å†…é‡‡é›†ï¼‰ï¼š

{chr(10).join(items_desc)}

è¦æ±‚è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "is_same_event": true/false,
  "confidence": 0.0-1.0,
  "reason": "åˆ¤æ–­ç†ç”±"
}}
"""
            
            # Token ä¼˜åŒ–ï¼šç¡®ä¿ prompt ä¸è¶…è¿‡é™åˆ¶
            prompt_tokens = self.token_manager.count_tokens(prompt)
            if prompt_tokens > self.max_prompt_tokens:
                logger.warning(
                    f"åŠæ—¥å½’å¹¶ prompt è¿‡é•¿ ({prompt_tokens} tokens)ï¼Œéœ€è¦æˆªæ–­"
                )
                prompt = self.token_manager.truncate_text(
                    prompt,
                    max_tokens=self.max_prompt_tokens
                )
                logger.info(f"æˆªæ–­å prompt: {self.token_manager.count_tokens(prompt)} tokens")
            
            try:
                # è°ƒç”¨ LLM
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ–°é—»äº‹ä»¶åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿åˆ¤æ–­ä¸åŒæ–°é—»æ˜¯å¦æŠ¥é“åŒä¸€äº‹ä»¶ã€‚"},
                    {"role": "user", "content": prompt}
                ]
                
                response = await self.llm_provider.chat_completion(
                    messages,
                    response_format="json",
                    max_tokens=self.max_completion_tokens  # ä½¿ç”¨é…ç½®çš„å€¼ï¼ˆ300ï¼‰
                )
                
                # è§£æç»“æœ
                result = json.loads(response["content"])
                
                # è®°å½• Token ä½¿ç”¨
                logger.info(
                    f"åŠæ—¥å½’å¹¶åˆ¤å®šå®Œæˆ - Prompt: {prompt_tokens} tokens, "
                    f"Completion: {response.get('usage', {}).get('completion_tokens', 0)} tokens, "
                    f"ç»“æœ: {result.get('is_same_event')} (ç½®ä¿¡åº¦: {result.get('confidence')})"
                )
                
                # è®°å½•åˆ¤å®šç»“æœ
                judgement = LLMJudgement(
                    type="halfday_merge",
                    status="success",
                    request={"items": [{"id": item.id, "title": item.title} for item in items]},
                    response=result,
                    latency_ms=0,  # TODO: è®°å½•å®é™…å»¶è¿Ÿ
                    tokens_prompt=response["usage"].get("prompt_tokens"),
                    tokens_completion=response["usage"].get("completion_tokens"),
                    provider=self.llm_provider.get_provider_name(),
                    model=self.llm_provider.model
                )
                self.db.add(judgement)
                # å…ˆflushç¡®ä¿IDç«‹å³åˆ†é…ï¼Œé¿å…å¹¶è¡Œå†²çª
                await self.db.flush()
                
                # å¦‚æœåˆ¤å®šä¸ºåŒä¸€äº‹ä»¶ï¼Œå½’å¹¶
                if result.get("is_same_event") and result.get("confidence", 0) >= 0.8:
                    group_id = f"halfday_{uuid.uuid4().hex[:8]}"
                    
                    # æ›´æ–°æ‰€æœ‰ item çš„ group_id
                    for item in items:
                        item.period_merge_group_id = group_id
                    
                    merge_groups.append({
                        "group_id": group_id,
                        "items": items,
                        "is_same_event": True,
                        "confidence": result.get("confidence", 0),
                        "reason": result.get("reason", "")
                    })
                else:
                    # ä¸æ˜¯åŒä¸€äº‹ä»¶ï¼Œæ‹†åˆ†ä¸ºå•ç‹¬çš„ç»„
                    for item in items:
                        group_id = f"halfday_{uuid.uuid4().hex[:8]}"
                        item.period_merge_group_id = group_id
                        merge_groups.append({
                            "group_id": group_id,
                            "items": [item],
                            "is_same_event": False,
                            "confidence": 1.0 - result.get("confidence", 0)
                        })
                
            except Exception as e:
                print(f"âŒ LLM åˆ¤å®šå¤±è´¥: {e}")
                # å¤±è´¥æ—¶æ¯ä¸ªitemå•ç‹¬æˆç»„
                for item in items:
                    group_id = f"halfday_{uuid.uuid4().hex[:8]}"
                    item.period_merge_group_id = group_id
                    merge_groups.append({
                        "group_id": group_id,
                        "items": [item],
                        "is_same_event": False,
                        "confidence": 0.5
                    })
        
        await self.db.commit()
        
        return merge_groups
    
    async def _filter_by_occurrence(
        self,
        merge_groups: List[Dict[str, Any]],
        min_occurrence: int = 2
    ) -> Tuple[List[SourceItem], List[SourceItem]]:
        """
        æ ¹æ®å‡ºç°æ¬¡æ•°ç­›é€‰
        
        Args:
            merge_groups: å½’å¹¶ç»„åˆ—è¡¨
            min_occurrence: æœ€å°å‡ºç°æ¬¡æ•°é˜ˆå€¼
            
        Returns:
            (ä¿ç•™çš„items, ä¸¢å¼ƒçš„items)
        """
        kept_items = []
        dropped_items = []
        
        for group in merge_groups:
            items = group["items"]
            occurrence = len(items)
            
            # æ›´æ–°å‡ºç°æ¬¡æ•°
            for item in items:
                item.occurrence_count = occurrence
            
            if occurrence >= min_occurrence:
                # ä¿ç•™
                for item in items:
                    item.merge_status = "pending_global_merge"
                    kept_items.append(item)
            else:
                # ä¸¢å¼ƒ
                for item in items:
                    item.merge_status = "discarded"
                    dropped_items.append(item)
        
        await self.db.commit()
        
        return kept_items, dropped_items
    
    async def _aggregate_heat(self, merge_groups: List[Dict[str, Any]]):
        """èšåˆæ¯ä¸ªå½’å¹¶ç»„çš„çƒ­åº¦"""
        for group in merge_groups:
            items = group["items"]
            
            if not items:
                continue
            
            # è®¡ç®—ç»„å†…çƒ­åº¦ï¼ˆä½¿ç”¨å¹³å‡å€¼æˆ–æœ€å¤§å€¼ï¼‰
            heat_values = [
                item.heat_normalized for item in items 
                if item.heat_normalized is not None
            ]
            
            if heat_values:
                group["avg_heat"] = sum(heat_values) / len(heat_values)
                group["max_heat"] = max(heat_values)
            else:
                group["avg_heat"] = 0.0
                group["max_heat"] = 0.0

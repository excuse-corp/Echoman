# Token ç®¡ç†æŒ‡å—

## ğŸ¯ èƒŒæ™¯

**qwen3-32b æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ä¸º 32k tokens**ã€‚åœ¨ä½¿ç”¨ AI æœåŠ¡æ—¶ï¼Œå¿…é¡»åˆç†åˆ†é… token é¢„ç®—ï¼Œé¿å…è¶…è¿‡ä¸Šä¸‹æ–‡é™åˆ¶å¯¼è‡´è¯·æ±‚å¤±è´¥ã€‚

---

## ğŸ“Š Token é¢„ç®—åˆ†é…ç­–ç•¥

### 32k ä¸Šä¸‹æ–‡åˆ†é…å»ºè®®

```
æ€»ä¸Šä¸‹æ–‡: 32,000 tokens
â”œâ”€â”€ å®‰å…¨è¾¹ç•Œ: 2,000 tokens      (6.25%)  é¢„ç•™ç»™ç³»ç»Ÿoverhead
â”œâ”€â”€ ç³»ç»ŸPrompt: 500-1,000 tokens (3%)    å›ºå®šçš„ç³»ç»ŸæŒ‡ä»¤
â”œâ”€â”€ ç”¨æˆ·æŸ¥è¯¢: 100-500 tokens    (1.5%)  ç”¨æˆ·é—®é¢˜
â”œâ”€â”€ æ£€ç´¢ä¸Šä¸‹æ–‡: 20,000 tokens   (62.5%) ä¸»è¦å†…å®¹
â””â”€â”€ ç”Ÿæˆå›å¤: 2,000-8,000 tokens(25%)   æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
```

### ä¸åŒåœºæ™¯çš„åˆ†é…

| åœºæ™¯ | ç³»ç»ŸPrompt | æŸ¥è¯¢ | ä¸Šä¸‹æ–‡ | å›å¤ | æ€»è®¡ |
|------|-----------|------|--------|------|------|
| **ç®€å•é—®ç­”** | 500 | 200 | 10,000 | 2,000 | 14,700 |
| **RAGå¯¹è¯** | 800 | 300 | 20,000 | 2,000 | 25,100 |
| **æ‘˜è¦ç”Ÿæˆ** | 600 | 100 | 15,000 | 5,000 | 20,700 |
| **åˆ†ç±»åˆ¤æ–­** | 400 | 200 | 5,000 | 500 | 6,100 |
| **é•¿æ–‡æœ¬ç”Ÿæˆ** | 500 | 200 | 8,000 | 8,000 | 16,700 |

---

## ğŸ› ï¸ ä½¿ç”¨ TokenManager

### åŸºæœ¬ä½¿ç”¨

```python
from app.utils.token_manager import TokenManager

# åˆ›å»º token ç®¡ç†å™¨
token_manager = TokenManager(model="qwen3-32b")

# è®¡ç®—æ–‡æœ¬ token æ•°
text = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬"
token_count = token_manager.count_tokens(text)
print(f"Token æ•°é‡: {token_count}")
```

### RAG ä¸Šä¸‹æ–‡ä¼˜åŒ–

```python
# ä¼˜åŒ– RAG ä¸Šä¸‹æ–‡
query = "æœ€è¿‘æœ‰ä»€ä¹ˆçƒ­ç‚¹äº‹ä»¶ï¼Ÿ"
context_chunks = [
    {"content": "æ–°é—»1çš„å†…å®¹...", "id": 1},
    {"content": "æ–°é—»2çš„å†…å®¹...", "id": 2},
    {"content": "æ–°é—»3çš„å†…å®¹...", "id": 3},
    # ... æ›´å¤šä¸Šä¸‹æ–‡
]

# è‡ªåŠ¨æˆªæ–­ï¼Œç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
optimized_chunks, stats = token_manager.optimize_rag_context(
    query=query,
    context_chunks=context_chunks,
    system_prompt_template="ä½ æ˜¯ä¸€ä¸ªçƒ­ç‚¹æ–°é—»åŠ©æ‰‹...",
    max_completion_tokens=2000
)

print(f"åŸå§‹å—æ•°: {stats['original_chunks']}")
print(f"ä¼˜åŒ–åå—æ•°: {stats['optimized_chunks']}")
print(f"ä½¿ç”¨çš„ä¸Šä¸‹æ–‡tokens: {stats['used_context_tokens']}")
print(f"å¯ç”¨çš„ä¸Šä¸‹æ–‡tokens: {stats['available_context_tokens']}")
```

### æ‰‹åŠ¨æˆªæ–­æ–‡æœ¬

```python
# æˆªæ–­é•¿æ–‡æœ¬
long_text = "å¾ˆé•¿å¾ˆé•¿çš„æ–‡æœ¬..." * 1000

# æˆªæ–­åˆ° 5000 tokensï¼Œä¿ç•™å¼€å¤´
truncated = token_manager.truncate_text(
    text=long_text,
    max_tokens=5000,
    keep_start=True
)

# æˆªæ–­åˆ° 3000 tokensï¼Œä¿ç•™ç»“å°¾
truncated_end = token_manager.truncate_text(
    text=long_text,
    max_tokens=3000,
    keep_start=False
)
```

### è®¡ç®—å¯ç”¨ä¸Šä¸‹æ–‡

```python
# è®¡ç®—è¿˜èƒ½ç”¨å¤šå°‘ tokens æ”¾ä¸Šä¸‹æ–‡
available = token_manager.calculate_available_context_tokens(
    system_prompt="ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹...",
    user_query="ç”¨æˆ·çš„é—®é¢˜",
    max_completion_tokens=2000
)

print(f"å¯ç”¨äºä¸Šä¸‹æ–‡çš„tokens: {available}")
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### settings.py ä¸­çš„ç›¸å…³é…ç½®

```python
# LLM è°ƒç”¨é…ç½®
llm_max_tokens: int = 2048              # å•æ¬¡å›å¤æœ€å¤§tokens
llm_context_limit: int = 32000          # æ€»ä¸Šä¸‹æ–‡é™åˆ¶
llm_safety_margin: int = 2000           # å®‰å…¨è¾¹ç•Œ

# RAG é…ç½®
rag_max_context_tokens: int = 20000     # RAGæœ€å¤§ä¸Šä¸‹æ–‡tokens
rag_max_completion_tokens: int = 2000   # ç”Ÿæˆå›å¤æœ€å¤§tokens
rag_enable_token_optimization: bool = True  # å¯ç”¨tokenä¼˜åŒ–
```

### ä¿®æ”¹é…ç½®

åˆ›å»º `.env` æ–‡ä»¶è¦†ç›–é»˜è®¤å€¼ï¼š

```bash
# .env
LLM_CONTEXT_LIMIT=32000
RAG_MAX_CONTEXT_TOKENS=20000
RAG_MAX_COMPLETION_TOKENS=2000
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ€»æ˜¯å¯ç”¨ Token ä¼˜åŒ–

```python
# âœ… å¥½çš„åšæ³•
if settings.rag_enable_token_optimization:
    optimized_chunks, _ = token_manager.optimize_rag_context(...)
else:
    optimized_chunks = context_chunks

# âŒ ä¸å¥½çš„åšæ³•
# ç›´æ¥ä½¿ç”¨æ‰€æœ‰æ£€ç´¢ç»“æœï¼Œå¯èƒ½è¶…è¿‡é™åˆ¶
context = "\n".join([chunk["content"] for chunk in all_chunks])
```

### 2. åˆ†çº§å¤„ç†é•¿æ–‡æœ¬

```python
# æ ¹æ®é‡è¦æ€§æ’åºï¼Œä¼˜å…ˆä¿ç•™é‡è¦å†…å®¹
sorted_chunks = sorted(
    context_chunks,
    key=lambda x: x.get("relevance_score", 0),
    reverse=True
)

# ç„¶åè¿›è¡Œ token ä¼˜åŒ–
optimized_chunks, _ = token_manager.optimize_rag_context(
    query=query,
    context_chunks=sorted_chunks,
    max_completion_tokens=2000
)
```

### 3. ç›‘æ§ Token ä½¿ç”¨æƒ…å†µ

```python
# è®°å½• token ç»Ÿè®¡
_, stats = token_manager.optimize_rag_context(...)

logger.info(f"Tokenä½¿ç”¨ç»Ÿè®¡: {stats}")

# å¦‚æœç»å¸¸æ¥è¿‘é™åˆ¶ï¼Œè€ƒè™‘è°ƒæ•´ç­–ç•¥
if stats['used_context_tokens'] > stats['available_context_tokens'] * 0.9:
    logger.warning("Tokenä½¿ç”¨æ¥è¿‘é™åˆ¶ï¼Œè€ƒè™‘å‡å°‘æ£€ç´¢æ•°é‡")
```

### 4. é’ˆå¯¹ä¸åŒä»»åŠ¡ä¼˜åŒ–

```python
# æ‘˜è¦ä»»åŠ¡ï¼šå¤šç»™ç”Ÿæˆç©ºé—´
summary_optimized, _ = token_manager.optimize_rag_context(
    query=query,
    context_chunks=chunks,
    max_completion_tokens=5000  # æ‘˜è¦éœ€è¦æ›´å¤šç”Ÿæˆç©ºé—´
)

# åˆ†ç±»ä»»åŠ¡ï¼šå°‘é‡ç”Ÿæˆå³å¯
classification_optimized, _ = token_manager.optimize_rag_context(
    query=query,
    context_chunks=chunks,
    max_completion_tokens=500  # åˆ†ç±»åªéœ€è¦ç®€çŸ­å›å¤
)
```

---

## ğŸš¨ å¸¸è§é™·é˜±

### âŒ é™·é˜± 1: å¿½ç•¥ç³»ç»Ÿ Prompt

```python
# é”™è¯¯ï¼šæ²¡æœ‰è®¡å…¥ç³»ç»Ÿ prompt çš„ tokens
max_context = 32000 - 2000  # åªå‡å»å®‰å…¨è¾¹ç•Œ

# æ­£ç¡®ï¼šè®¡å…¥æ‰€æœ‰å›ºå®šå†…å®¹
system_prompt_tokens = token_manager.count_tokens(system_prompt)
max_context = 32000 - 2000 - system_prompt_tokens - query_tokens - max_completion_tokens
```

### âŒ é™·é˜± 2: è¿‡åº¦ä¼°ç®—

```python
# é”™è¯¯ï¼šè¿‡åº¦ä¿å®ˆï¼Œæµªè´¹ä¸Šä¸‹æ–‡ç©ºé—´
max_context = 32000 // 4  # åªç”¨ 25%

# æ­£ç¡®ï¼šåˆç†åˆ†é…
max_context = 32000 - safety_margin - fixed_costs
```

### âŒ é™·é˜± 3: æ²¡æœ‰é™çº§ç­–ç•¥

```python
# é”™è¯¯ï¼šæ£€ç´¢å¤±è´¥å°±ç›´æ¥æŠ¥é”™
if not context_chunks:
    raise ValueError("æ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹")

# æ­£ç¡®ï¼šæœ‰é™çº§æ–¹æ¡ˆ
if not context_chunks:
    return fallback_answer(query)
```

---

## ğŸ“Š Token è®¡æ•°è§„åˆ™

### ä¸­è‹±æ–‡æ··åˆä¼°ç®—

```python
# è§„åˆ™ï¼š
# - çº¯è‹±æ–‡ï¼šçº¦ 4 å­—ç¬¦ = 1 token
# - çº¯ä¸­æ–‡ï¼šçº¦ 1.5 å­—ç¬¦ = 1 token
# - æ··åˆï¼šçº¦ 2 å­—ç¬¦ = 1 tokenï¼ˆä¿å®ˆä¼°è®¡ï¼‰

# ç¤ºä¾‹
text1 = "Hello world"  # çº¦ 3 tokens
text2 = "ä½ å¥½ä¸–ç•Œ"    # çº¦ 3 tokens
text3 = "Hello ä¸–ç•Œ"  # çº¦ 3 tokens
```

### ç²¾ç¡® vs ä¼°ç®—

```python
# ç²¾ç¡®è®¡æ•°ï¼ˆéœ€è¦ tiktokenï¼‰
token_manager = TokenManager(model="qwen3-32b")
exact_count = token_manager.count_tokens(text)

# å¿«é€Ÿä¼°ç®—ï¼ˆä¸ä¾èµ– tiktokenï¼‰
from app.utils.token_manager import estimate_tokens_simple
approx_count = estimate_tokens_simple(text)

# ä¼°ç®—è¯¯å·®é€šå¸¸åœ¨ Â±10% ä»¥å†…
```

---

## ğŸ“ è¿›é˜¶æŠ€å·§

### 1. åŠ¨æ€è°ƒæ•´æ£€ç´¢æ•°é‡

```python
# æ ¹æ® token é™åˆ¶åŠ¨æ€è°ƒæ•´æ£€ç´¢æ•°é‡
def adaptive_retrieval(query, initial_topk=10):
    token_manager = TokenManager(model="qwen3-32b")
    available_tokens = token_manager.calculate_available_context_tokens(
        system_prompt=SYSTEM_PROMPT,
        user_query=query,
        max_completion_tokens=2000
    )
    
    # ä¼°ç®—æ¯ä¸ªchunkçš„å¹³å‡tokenæ•°
    avg_chunk_tokens = 2000  # æ ¹æ®å®é™…æ•°æ®è°ƒæ•´
    
    # åŠ¨æ€è°ƒæ•´topk
    adjusted_topk = min(initial_topk, available_tokens // avg_chunk_tokens)
    
    return retrieve_chunks(query, topk=adjusted_topk)
```

### 2. æ¸è¿›å¼æ·»åŠ ä¸Šä¸‹æ–‡

```python
# ä¼˜å…ˆçº§æ’åºåï¼Œé€ä¸ªæ·»åŠ ç›´åˆ°æ¥è¿‘é™åˆ¶
def progressive_context_building(chunks, max_tokens):
    token_manager = TokenManager(model="qwen3-32b")
    selected_chunks = []
    total_tokens = 0
    
    for chunk in sorted_chunks_by_priority(chunks):
        chunk_tokens = token_manager.count_tokens(chunk["content"])
        
        if total_tokens + chunk_tokens <= max_tokens * 0.95:  # ç•™5%ç¼“å†²
            selected_chunks.append(chunk)
            total_tokens += chunk_tokens
        else:
            break
    
    return selected_chunks, total_tokens
```

### 3. åˆ†æ®µå¤„ç†è¶…é•¿æ–‡æ¡£

```python
# å¯¹äºè¶…é•¿æ–‡æ¡£ï¼Œåˆ†æ®µå¤„ç†
def process_long_document(document, chunk_size=15000):
    token_manager = TokenManager(model="qwen3-32b")
    
    # åˆ†æ®µ
    segments = []
    current_segment = ""
    current_tokens = 0
    
    for paragraph in document.split("\n\n"):
        para_tokens = token_manager.count_tokens(paragraph)
        
        if current_tokens + para_tokens <= chunk_size:
            current_segment += paragraph + "\n\n"
            current_tokens += para_tokens
        else:
            if current_segment:
                segments.append(current_segment)
            current_segment = paragraph + "\n\n"
            current_tokens = para_tokens
    
    if current_segment:
        segments.append(current_segment)
    
    # åˆ†åˆ«å¤„ç†æ¯æ®µ
    results = []
    for segment in segments:
        result = process_segment(segment)
        results.append(result)
    
    # åˆå¹¶ç»“æœ
    return merge_results(results)
```

---

## ğŸ“– ç›¸å…³æ–‡æ¡£

- [RAG æœåŠ¡å®ç°](../services/rag_service.py)
- [é…ç½®è¯´æ˜](../config/settings.py)
- [LLM Provider](../services/llm/)

---

**æœ€åæ›´æ–°**: 2025-10-30  
**é€‚ç”¨æ¨¡å‹**: qwen3-32b (32k context)


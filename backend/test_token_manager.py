#!/usr/bin/env python3
"""
Token Manager æµ‹è¯•è„šæœ¬

æµ‹è¯• TokenManager çš„å„é¡¹åŠŸèƒ½
"""
import sys
sys.path.insert(0, '/root/ren/Echoman/backend')

from app.utils.token_manager import TokenManager, estimate_tokens_simple


def test_token_counting():
    """æµ‹è¯• Token è®¡æ•°"""
    print("=" * 60)
    print("æµ‹è¯• 1: Token è®¡æ•°")
    print("=" * 60)
    
    tm = TokenManager(model="qwen3-32b")
    
    texts = [
        "Hello World",
        "ä½ å¥½ä¸–ç•Œ",
        "This is a test è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•",
        "A" * 1000,
        "ä¸­" * 1000,
    ]
    
    for text in texts:
        token_count = tm.count_tokens(text)
        simple_estimate = estimate_tokens_simple(text)
        
        print(f"\næ–‡æœ¬: {text[:50]}{'...' if len(text) > 50 else ''}")
        print(f"  é•¿åº¦: {len(text)} å­—ç¬¦")
        print(f"  ç²¾ç¡®è®¡æ•°: {token_count} tokens")
        print(f"  ç®€å•ä¼°ç®—: {simple_estimate} tokens")
        print(f"  å·®å¼‚: {abs(token_count - simple_estimate)} tokens")


def test_text_truncation():
    """æµ‹è¯•æ–‡æœ¬æˆªæ–­"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: æ–‡æœ¬æˆªæ–­")
    print("=" * 60)
    
    tm = TokenManager(model="qwen3-32b")
    
    long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬ã€‚" * 100
    
    print(f"\nåŸå§‹æ–‡æœ¬: {len(long_text)} å­—ç¬¦")
    print(f"åŸå§‹tokens: {tm.count_tokens(long_text)}")
    
    # æˆªæ–­åˆ° 100 tokens
    truncated = tm.truncate_text(long_text, max_tokens=100, keep_start=True)
    
    print(f"\næˆªæ–­åæ–‡æœ¬: {len(truncated)} å­—ç¬¦")
    print(f"æˆªæ–­åtokens: {tm.count_tokens(truncated)}")
    print(f"æˆªæ–­å†…å®¹é¢„è§ˆ: {truncated[:100]}...")


def test_context_optimization():
    """æµ‹è¯•ä¸Šä¸‹æ–‡ä¼˜åŒ–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: RAG ä¸Šä¸‹æ–‡ä¼˜åŒ–")
    print("=" * 60)
    
    tm = TokenManager(model="qwen3-32b")
    
    # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
    context_chunks = [
        {"id": 1, "content": "æ–°é—»1: " + "è¿™æ˜¯ç¬¬ä¸€æ¡æ–°é—»çš„å†…å®¹ã€‚" * 50},
        {"id": 2, "content": "æ–°é—»2: " + "è¿™æ˜¯ç¬¬äºŒæ¡æ–°é—»çš„å†…å®¹ã€‚" * 50},
        {"id": 3, "content": "æ–°é—»3: " + "è¿™æ˜¯ç¬¬ä¸‰æ¡æ–°é—»çš„å†…å®¹ã€‚" * 50},
        {"id": 4, "content": "æ–°é—»4: " + "è¿™æ˜¯ç¬¬å››æ¡æ–°é—»çš„å†…å®¹ã€‚" * 50},
        {"id": 5, "content": "æ–°é—»5: " + "è¿™æ˜¯ç¬¬äº”æ¡æ–°é—»çš„å†…å®¹ã€‚" * 50},
    ]
    
    query = "æœ€è¿‘æœ‰ä»€ä¹ˆçƒ­ç‚¹æ–°é—»ï¼Ÿ"
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ–°é—»åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„æ–°é—»å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
    
    print(f"\næŸ¥è¯¢: {query}")
    print(f"ç³»ç»ŸPrompt: {system_prompt}")
    print(f"åŸå§‹å—æ•°: {len(context_chunks)}")
    print(f"åŸå§‹æ€»tokens: {sum(tm.count_tokens(c['content']) for c in context_chunks)}")
    
    # ä¼˜åŒ–ä¸Šä¸‹æ–‡
    optimized_chunks, stats = tm.optimize_rag_context(
        query=query,
        context_chunks=context_chunks,
        system_prompt_template=system_prompt,
        max_completion_tokens=2000
    )
    
    print(f"\nä¼˜åŒ–åå—æ•°: {len(optimized_chunks)}")
    print(f"ä¼˜åŒ–åæ€»tokens: {sum(tm.count_tokens(c['content']) for c in optimized_chunks)}")
    
    print("\nToken ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")


def test_available_tokens():
    """æµ‹è¯•å¯ç”¨ Token è®¡ç®—"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: å¯ç”¨ Token è®¡ç®—")
    print("=" * 60)
    
    tm = TokenManager(model="qwen3-32b")
    
    scenarios = [
        {
            "name": "ç®€å•é—®ç­”",
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚",
            "query": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "max_completion": 500
        },
        {
            "name": "RAGå¯¹è¯",
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢çš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚",
            "query": "è¯·è¯¦ç»†è§£é‡Šä¸€ä¸‹ç›¸å…³å†…å®¹ï¼Œå¹¶ç»™å‡ºä½ çš„åˆ†æã€‚",
            "max_completion": 2000
        },
        {
            "name": "é•¿æ–‡æœ¬ç”Ÿæˆ",
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæ–‡ç« ç”ŸæˆåŠ©æ‰‹ã€‚",
            "query": "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½å‘å±•çš„æ–‡ç« ã€‚",
            "max_completion": 8000
        }
    ]
    
    for scenario in scenarios:
        available = tm.calculate_available_context_tokens(
            system_prompt=scenario["system_prompt"],
            user_query=scenario["query"],
            max_completion_tokens=scenario["max_completion"]
        )
        
        print(f"\nåœºæ™¯: {scenario['name']}")
        print(f"  ç³»ç»ŸPrompt tokens: {tm.count_tokens(scenario['system_prompt'])}")
        print(f"  æŸ¥è¯¢ tokens: {tm.count_tokens(scenario['query'])}")
        print(f"  é¢„ç•™ç”Ÿæˆ tokens: {scenario['max_completion']}")
        print(f"  å¯ç”¨ä¸Šä¸‹æ–‡ tokens: {available}")
        print(f"  å æ¯”: {available / tm.context_limit * 100:.1f}%")


def test_model_limits():
    """æµ‹è¯•ä¸åŒæ¨¡å‹çš„é™åˆ¶"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: ä¸åŒæ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶")
    print("=" * 60)
    
    models = [
        "qwen3-32b",
        "gpt-4",
        "gpt-4o",
        "claude-3-opus"
    ]
    
    for model in models:
        tm = TokenManager(model=model)
        print(f"\næ¨¡å‹: {model}")
        print(f"  ä¸Šä¸‹æ–‡é™åˆ¶: {tm.context_limit:,} tokens")
        print(f"  å®‰å…¨è¾¹ç•Œ: {tm.SAFETY_MARGIN:,} tokens")
        print(f"  å¯ç”¨ä¸Šä¸‹æ–‡: {tm.context_limit - tm.SAFETY_MARGIN:,} tokens")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ§ª " * 20)
    print("Token Manager åŠŸèƒ½æµ‹è¯•")
    print("ğŸ§ª " * 20)
    
    try:
        test_token_counting()
        test_text_truncation()
        test_context_optimization()
        test_available_tokens()
        test_model_limits()
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())


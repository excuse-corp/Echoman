# RAG å¯¹è¯ç³»ç»Ÿå®ç°é€»è¾‘

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
2. [æ ¸å¿ƒæµç¨‹](#æ ¸å¿ƒæµç¨‹)
3. [ä¸¤ç§å¯¹è¯æ¨¡å¼](#ä¸¤ç§å¯¹è¯æ¨¡å¼)
4. [å…³é”®ç»„ä»¶](#å…³é”®ç»„ä»¶)
5. [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‰ç«¯è¯·æ±‚    â”‚
â”‚ (SSE/JSON)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API å±‚            â”‚
â”‚ /api/v1/chat/ask   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG Service       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ 1. å‚æ•°éªŒè¯     â”‚ â”‚
â”‚ â”‚ 2. æ£€ç´¢ä¸Šä¸‹æ–‡   â”‚ â”‚
â”‚ â”‚ 3. Tokenä¼˜åŒ–    â”‚ â”‚
â”‚ â”‚ 4. æ„å»ºPrompt   â”‚ â”‚
â”‚ â”‚ 5. LLMè°ƒç”¨      â”‚ â”‚
â”‚ â”‚ 6. è¿”å›ç»“æœ     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼               â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector   â”‚    â”‚ Token    â”‚  â”‚ LLM      â”‚
â”‚ Search   â”‚    â”‚ Manager  â”‚  â”‚ Provider â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ æ ¸å¿ƒæµç¨‹

### 1. **æ¥æ”¶è¯·æ±‚** (`/api/v1/chat/ask`)

```python
POST /api/v1/chat/ask
{
    "query": "ç”¨æˆ·é—®é¢˜",
    "mode": "topic" | "global",
    "topic_id": 123,  // topicæ¨¡å¼å¿…éœ€
    "stream": true    // æ˜¯å¦æµå¼è¾“å‡º
}
```

### 2. **RAG å¤„ç†æµç¨‹**

```python
async def ask(query, mode, topic_id, chat_id):
    # â‘  å‚æ•°éªŒè¯
    if mode == "topic" and not topic_id:
        raise ValueError("topicæ¨¡å¼éœ€è¦æä¾›topic_id")
    
    # â‘¡ æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    if mode == "topic":
        context, citations = await _retrieve_topic_context(db, topic_id, query)
    else:
        context, citations = await _retrieve_global_context(db, query)
    
    # â‘¢ é™çº§å¤„ç†
    if not context:
        return await _fallback_answer(query, mode)
    
    # â‘£ æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
    formatted_context = _format_context_chunks(context)
    
    # â‘¤ Token ä¼˜åŒ–ï¼ˆç¡®ä¿ä¸è¶…è¿‡32ké™åˆ¶ï¼‰
    optimized_context, token_stats = token_manager.optimize_rag_context(
        query=query,
        context_chunks=formatted_context,
        system_prompt=_get_system_prompt(mode),
        max_completion_tokens=2000
    )
    
    # â‘¥ æ„å»º RAG Prompt
    prompt = _build_rag_prompt(query, optimized_context, mode)
    
    # â‘¦ è°ƒç”¨ LLM
    response = await llm_provider.chat_completion(
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=2000
    )
    
    # â‘§ è§£æå’Œè¿”å›
    answer = _parse_answer(response)
    return {
        "answer": answer,
        "citations": citations,
        "diagnostics": {...}
    }
```

---

## ğŸ¯ ä¸¤ç§å¯¹è¯æ¨¡å¼

### **æ¨¡å¼ 1: Topic æ¨¡å¼ï¼ˆäº‹ä»¶æ¨¡å¼ï¼‰**

**ä½¿ç”¨åœºæ™¯**ï¼šé’ˆå¯¹ç‰¹å®šçƒ­ç‚¹äº‹ä»¶çš„æ·±åº¦é—®ç­”

**æ£€ç´¢é€»è¾‘**ï¼š

```python
async def _retrieve_topic_context(db, topic_id, query):
    # 1. è·å–ä¸»é¢˜åŸºæœ¬ä¿¡æ¯
    topic = await db.get(Topic, topic_id)
    
    # 2. è·å–ä¸»é¢˜æ‘˜è¦
    summary = await db.get(Summary, topic.summary_id)
    
    # 3. å‘é‡æ£€ç´¢æœ€ç›¸å…³çš„èŠ‚ç‚¹ï¼ˆTopK=5ï¼‰
    query_embedding = await _get_query_embedding(query)
    relevant_nodes = await _vector_search_nodes(
        db, topic_id, query_embedding, limit=5
    )
    
    # 4. æ„é€ ä¸Šä¸‹æ–‡
    context = []
    
    # æ·»åŠ æ‘˜è¦
    if summary:
        context.append({
            "type": "summary",
            "content": summary.content
        })
    
    # æ·»åŠ ç›¸å…³èŠ‚ç‚¹
    for node in relevant_nodes:
        context.append({
            "type": "node",
            "platform": node.source_item.platform,
            "title": node.source_item.title,
            "summary": node.source_item.summary,
            "url": node.source_item.url
        })
    
    return context, citations
```

**å‘é‡æ£€ç´¢**ï¼š

```sql
-- ä½¿ç”¨ pgvector è¿›è¡Œä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
SELECT n.id, (e.vector <=> query_vector::vector) as distance
FROM topic_nodes n
JOIN embeddings e ON e.object_type = 'node' AND e.object_id = n.id
WHERE n.topic_id = :topic_id
  AND (e.vector <=> query_vector::vector) < 0.7  -- ç›¸ä¼¼åº¦é˜ˆå€¼
ORDER BY distance ASC
LIMIT 5
```

### **æ¨¡å¼ 2: Global æ¨¡å¼ï¼ˆè‡ªç”±æ¨¡å¼ï¼‰**

**ä½¿ç”¨åœºæ™¯**ï¼šè·¨äº‹ä»¶çš„å…¨å±€æ£€ç´¢å’Œå¯¹æ¯”

**æ£€ç´¢é€»è¾‘**ï¼š

```python
async def _retrieve_global_context(db, query):
    # 1. å‘é‡æ£€ç´¢ç›¸å…³ä¸»é¢˜ï¼ˆTopK=10ï¼‰
    query_embedding = await _get_query_embedding(query)
    relevant_topics = await _vector_search_topics(
        db, query_embedding, limit=10
    )
    
    # 2. æ„é€ ä¸Šä¸‹æ–‡
    context = []
    
    for topic in relevant_topics:
        # è·å–ä¸»é¢˜æ‘˜è¦
        summary = await db.get(Summary, topic.summary_id)
        if summary:
            context.append({
                "type": "topic_summary",
                "topic_title": topic.title_key,
                "content": summary.content,
                "intensity": topic.intensity_total,
                "first_seen": topic.first_seen
            })
        
        # è·å–è¯¥ä¸»é¢˜çš„ä»£è¡¨æ€§èŠ‚ç‚¹
        nodes = await _get_latest_nodes(db, topic.id, limit=2)
        for node in nodes:
            context.append({
                "type": "node",
                "topic_title": topic.title_key,
                "platform": node.source_item.platform,
                "title": node.source_item.title,
                "summary": node.source_item.summary
            })
    
    return context, citations
```

**å‘é‡æ£€ç´¢**ï¼š

```sql
-- æ£€ç´¢ä¸»é¢˜æ‘˜è¦çš„å‘é‡ç›¸ä¼¼åº¦
SELECT t.id, (e.vector <=> query_vector::vector) as distance
FROM topics t
JOIN summaries s ON s.id = t.summary_id
JOIN embeddings e ON e.object_type = 'topic_summary' AND e.object_id = s.id
WHERE t.status = 'active'
ORDER BY distance ASC
LIMIT 10
```

---

## ğŸ”§ å…³é”®ç»„ä»¶

### 1. **Token Manager** - Token ç®¡ç†å’Œä¼˜åŒ–

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- è®¡ç®— Token æ•°é‡ï¼ˆä½¿ç”¨ tiktokenï¼‰
- ä¼˜åŒ–ä¸Šä¸‹æ–‡é¿å…è¶…è¿‡æ¨¡å‹é™åˆ¶ï¼ˆ32kï¼‰
- æ™ºèƒ½æˆªæ–­å’Œä¿ç•™æœ€ç›¸å…³å†…å®¹

```python
class TokenManager:
    MODEL_CONTEXT_LIMITS = {
        "qwen3-32b": 32000,
        "gpt-4o": 128000,
        ...
    }
    
    def optimize_rag_context(
        self,
        query: str,
        context_chunks: List[Dict],
        system_prompt_template: str,
        max_completion_tokens: int = 2000
    ):
        """
        ä¼˜åŒ– RAG ä¸Šä¸‹æ–‡
        
        è®¡ç®—é€»è¾‘ï¼š
        available_tokens = model_limit - system_tokens - query_tokens - max_completion
        
        è¿”å›ï¼š
        - optimized_chunks: ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡å—
        - token_stats: Token ä½¿ç”¨ç»Ÿè®¡
        """
        # 1. è®¡ç®—å„éƒ¨åˆ† token
        query_tokens = self.count_tokens(query)
        system_tokens = self.count_tokens(system_prompt_template)
        
        # 2. è®¡ç®—å¯ç”¨ token
        available = (
            self.context_limit 
            - self.SAFETY_MARGIN 
            - system_tokens 
            - query_tokens 
            - max_completion_tokens
        )
        
        # 3. æ ¹æ®å¯ç”¨ token æˆªæ–­ä¸Šä¸‹æ–‡
        optimized_chunks = []
        used_tokens = 0
        
        for chunk in context_chunks:
            chunk_tokens = self.count_tokens(chunk["content"])
            if used_tokens + chunk_tokens <= available:
                optimized_chunks.append(chunk)
                used_tokens += chunk_tokens
            else:
                # æˆªæ–­æœ€åä¸€ä¸ªå—
                remaining = available - used_tokens
                if remaining > 100:  # è‡³å°‘ä¿ç•™100 tokens
                    truncated = self.truncate_text(
                        chunk["content"], 
                        remaining
                    )
                    optimized_chunks.append({
                        **chunk, 
                        "content": truncated
                    })
                break
        
        return optimized_chunks, {
            "original_chunks": len(context_chunks),
            "optimized_chunks": len(optimized_chunks),
            "used_context_tokens": used_tokens,
            "available_context_tokens": available
        }
```

### 2. **LLM Provider** - å¤šæ¨¡å‹æ”¯æŒ

**æ”¯æŒçš„æä¾›å•†**ï¼š
- Qwen (é€šä¹‰åƒé—®)
- OpenAI
- Azure OpenAI
- OpenAI Compatible (ä»»ä½•å…¼å®¹æ¥å£)

```python
class BaseLLMProvider:
    async def chat_completion(
        self, 
        messages: List[Dict], 
        temperature: float, 
        max_tokens: int
    ) -> str:
        """æ ‡å‡†å¯¹è¯å®Œæˆæ¥å£"""
        
    async def chat_completion_stream(
        self,
        messages: List[Dict],
        temperature: float,
        max_tokens: int
    ) -> AsyncGenerator[Dict, None]:
        """æµå¼å¯¹è¯å®Œæˆæ¥å£ï¼ˆSSEï¼‰"""
        
    async def embedding(self, text: str) -> List[float]:
        """æ–‡æœ¬å‘é‡åŒ–æ¥å£"""
```

**å·¥å‚æ¨¡å¼åˆ›å»º**ï¼š

```python
def get_llm_provider(provider_name: str, model: str):
    if provider_name == "qwen":
        return QwenProvider(
            model=model,
            api_key=settings.qwen_api_key,
            base_url=settings.qwen_api_base
        )
    elif provider_name == "openai":
        return OpenAIProvider(...)
    # ...
```

### 3. **å‘é‡æ£€ç´¢** - pgvector å®ç°

**æ•°æ®åº“è®¾ç½®**ï¼š

```sql
-- å¯ç”¨ pgvector æ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;

-- embeddings è¡¨
CREATE TABLE embeddings (
    id BIGSERIAL PRIMARY KEY,
    object_type VARCHAR(50),  -- 'node', 'topic_summary'
    object_id BIGINT,
    vector vector(1536),      -- OpenAI ada-002 ç»´åº¦
    model VARCHAR(100),
    created_at TIMESTAMP
);

-- åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆHNSW ç®—æ³•ï¼Œå¿«é€Ÿè¿‘ä¼¼æœç´¢ï¼‰
CREATE INDEX idx_embeddings_vector ON embeddings 
USING hnsw (vector vector_cosine_ops);
```

**æ£€ç´¢æŸ¥è¯¢**ï¼š

```python
# ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢ (pgvector)
stmt = text("""
    SELECT n.id, (e.vector <=> :query_vector::vector) as distance
    FROM topic_nodes n
    JOIN embeddings e ON e.object_type = 'node' AND e.object_id = n.id
    WHERE n.topic_id = :topic_id
    ORDER BY distance ASC
    LIMIT :limit
""")

result = await db.execute(stmt, {
    "query_vector": f"[{','.join(map(str, query_embedding))}]",
    "topic_id": topic_id,
    "limit": 5
})
```

**ç›¸ä¼¼åº¦è®¡ç®—**ï¼š
- `<=>` è¿ç®—ç¬¦ï¼šä½™å¼¦è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰
- é˜ˆå€¼ï¼š0.7ï¼ˆè·ç¦» > 0.7 çš„ä¼šè¢«è¿‡æ»¤ï¼‰

---

## ğŸš€ æŠ€æœ¯ç»†èŠ‚

### 1. **æµå¼è¾“å‡ºï¼ˆSSEï¼‰**

```python
async def ask_stream(db, query, mode, topic_id):
    """
    SSE äº‹ä»¶æµæ ¼å¼ï¼š
    event: token
    data: {"content": "æ–‡"}
    
    event: token  
    data: {"content": "å­—"}
    
    event: citations
    data: {"citations": [...]}
    
    event: done
    data: {"diagnostics": {...}}
    """
    # æ£€ç´¢ä¸Šä¸‹æ–‡
    context, citations = await _retrieve_context(...)
    
    # æ„å»º prompt
    prompt = _build_rag_prompt(query, context, mode)
    
    # æµå¼è°ƒç”¨ LLM
    full_answer = ""
    async for chunk in llm_provider.chat_completion_stream(
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    ):
        content = chunk.get("content", "")
        if content:
            full_answer += content
            
            # å‘é€ token äº‹ä»¶
            yield {
                "type": "token",
                "data": {"content": content}
            }
    
    # å‘é€å¼•ç”¨
    yield {
        "type": "citations",
        "data": {"citations": citations}
    }
    
    # å‘é€å®Œæˆä¿¡å·
    yield {
        "type": "done",
        "data": {"diagnostics": {...}}
    }
```

**å‰ç«¯æ¥æ”¶**ï¼š

```typescript
const response = await fetch('/api/v1/chat/ask', {
    method: 'POST',
    body: JSON.stringify({ query, mode, stream: true })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();
let buffer = "";
let currentEvent = "";

while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    
    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split("\n");
    buffer = lines.pop() || "";
    
    for (const line of lines) {
        if (line.startsWith("event:")) {
            currentEvent = line.substring(6).trim();
        }
        if (line.startsWith("data:")) {
            const data = JSON.parse(line.substring(5).trim());
            
            if (currentEvent === "token") {
                onToken(data.content);  // é€å­—æ˜¾ç¤º
            } else if (currentEvent === "citations") {
                onCitations(data.citations);
            } else if (currentEvent === "done") {
                onDone(data.diagnostics);
            }
        }
    }
}
```

### 2. **Prompt å·¥ç¨‹**

**Topic æ¨¡å¼ Prompt**ï¼š

```
è¯·åŸºäºä»¥ä¸‹ä¸»é¢˜å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ã€å‚è€ƒå†…å®¹ã€‘
ã€ä¸»é¢˜æ‘˜è¦ã€‘
ç‹ä¼ å›è·ä¸œäº¬ç”µå½±èŠ‚å½±å¸...

1. [ä»Šæ—¥å¤´æ¡] 2025/11/04
   ç‹ä¼ å›è·ä¸œäº¬ç”µå½±èŠ‚å½±å¸
   æ—¥æœ¬ä¸œäº¬ç”µå½±èŠ‚é—­å¹•ï¼Œç‹ä¼ å›å‡­å€Ÿ...

2. [å¾®åš] 2025/11/04
   ...

ã€ç”¨æˆ·é—®é¢˜ã€‘
ç‹ä¼ å›åœ¨ä¸œäº¬ç”µå½±èŠ‚è·å¾—äº†ä»€ä¹ˆå¥–é¡¹ï¼Ÿ

è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœå‚è€ƒå†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œæ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´ã€æœ‰æ¡ç†
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ¥æºï¼ˆå¦‚"æ ¹æ®å¾®åšæ¶ˆæ¯..."ï¼‰

è¯·å›ç­”ï¼š
```

**Global æ¨¡å¼ Prompt**ï¼š

```
è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ã€å‚è€ƒå†…å®¹ã€‘ï¼ˆæŒ‰ç›¸å…³æ€§æ’åºï¼‰
ã€ä¸»é¢˜1ï¼šç‹ä¼ å›è·ä¸œäº¬ç”µå½±èŠ‚å½±å¸ã€‘
æ‘˜è¦ï¼šç‹ä¼ å›è·å¾—ä¸œäº¬ç”µå½±èŠ‚æœ€ä½³ç”·æ¼”å‘˜...
å›å£°å¼ºåº¦ï¼š8250

1. [ä»Šæ—¥å¤´æ¡] 2025/11/04
   ç‹ä¼ å›è·ä¸œäº¬ç”µå½±èŠ‚å½±å¸
   ...

ã€ä¸»é¢˜2ï¼š2026æ˜¥èŠ‚æ”¾9å¤©å‡ã€‘
æ‘˜è¦ï¼šå›½åŠ¡é™¢å‘å¸ƒ2026å¹´èŠ‚å‡æ—¥å®‰æ’...
å›å£°å¼ºåº¦ï¼š6800

ã€ç”¨æˆ·é—®é¢˜ã€‘
æœ€è¿‘æœ‰å“ªäº›çƒ­ç‚¹äº‹ä»¶ï¼Ÿ

è¦æ±‚ï¼š
1. ç»¼åˆå¤šä¸ªä¸»é¢˜çš„ä¿¡æ¯å›ç­”
2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
3. å›ç­”è¦å‡†ç¡®ã€å®¢è§‚ã€æœ‰æ¡ç†
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„ä¸»é¢˜æˆ–æ¥æº

è¯·å›ç­”ï¼š
```

### 3. **å¯¹è¯å†å²ç®¡ç†**

**è®¾è®¡ç†å¿µ**ï¼šå¯¹è¯ä¸Šä¸‹æ–‡åœ¨å‰ç«¯ç»´æŠ¤ï¼Œæ— éœ€æŒä¹…åŒ–å­˜å‚¨

**å‰ç«¯å®ç°**ï¼š

```typescript
// ConversationConsole.tsx
const [messages, setMessages] = useState<Message[]>([]);

// æ¶ˆæ¯ç±»å‹
interface Message {
  id: string;
  role: "user" | "assistant" | "timeline";
  text: string;
  timestamp: Date;
  // ä»…ç”¨äºæ—¶é—´çº¿æ¶ˆæ¯
  summary?: string;
  keyPoints?: string[];
  timelineNodes?: TimelineNode[];
}

// æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
const addUserMessage = (text: string) => {
  setMessages(prev => [...prev, {
    id: Date.now().toString(),
    role: "user",
    text,
    timestamp: new Date()
  }]);
};

// æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ï¼ˆæµå¼ï¼‰
const addAssistantMessage = (id: string, chunk: string) => {
  setMessages(prev => {
    const lastMsg = prev[prev.length - 1];
    if (lastMsg?.id === id && lastMsg.role === "assistant") {
      // è¿½åŠ åˆ°ç°æœ‰æ¶ˆæ¯
      return [...prev.slice(0, -1), {
        ...lastMsg,
        text: lastMsg.text + chunk
      }];
    } else {
      // åˆ›å»ºæ–°æ¶ˆæ¯
      return [...prev, {
        id,
        role: "assistant",
        text: chunk,
        timestamp: new Date()
      }];
    }
  });
};

// åˆ·æ–°å¯¹è¯ï¼ˆé‡ç½®åˆ°åˆå§‹çŠ¶æ€ï¼‰
const handleRefresh = () => {
  setMessages(buildInitialMessages());
  setInput("");
};
```

**ä¼˜ç‚¹**ï¼š
- âœ… **è½»é‡çº§**ï¼šæ— éœ€æ•°æ®åº“å­˜å‚¨ï¼Œå‡å°‘åç«¯è´Ÿæ‹…
- âœ… **å®æ—¶æ€§**ï¼šå¯¹è¯çŠ¶æ€å³æ—¶å“åº”ï¼Œæ— å»¶è¿Ÿ
- âœ… **éšç§æ€§**ï¼šå¯¹è¯ä¸æŒä¹…åŒ–ï¼Œä¿æŠ¤ç”¨æˆ·éšç§
- âœ… **çµæ´»æ€§**ï¼šåˆ·æ–°å³å¯é‡ç½®ï¼Œç”¨æˆ·ä½“éªŒæ›´å¥½

**æ•°æ®åº“è¡¨**ï¼ˆä¿ç•™ä½†ä¸ä½¿ç”¨ï¼‰ï¼š

```python
# chats è¡¨ï¼ˆä¿ç•™ç”¨äºæœªæ¥æ‰©å±•ï¼Œå¦‚å¯¹è¯è®°å½•åŠŸèƒ½ï¼‰
chats:
  - id: ä¼šè¯ID
  - mode: 'topic' | 'global'
  - topic_id: å…³è”ä¸»é¢˜IDï¼ˆtopicæ¨¡å¼ï¼‰
  - created_at: åˆ›å»ºæ—¶é—´

# å½“å‰ç‰ˆæœ¬ä¸å†å­˜å‚¨ chat_messages å’Œ citations
```

### 4. **é™çº§ç­–ç•¥**

```python
# 1. å‘é‡æ£€ç´¢å¤±è´¥ -> ä½¿ç”¨æ—¶é—´æ’åº
if not query_embedding:
    relevant_nodes = await _get_latest_nodes(db, topic_id, limit=5)

# 2. æ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ -> è¿”å›å‹å¥½æç¤º
if not context:
    return {
        "answer": "æŠ±æ­‰ï¼Œæš‚æ—¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
        "citations": [],
        "diagnostics": {"fallback": True}
    }

# 3. LLM è°ƒç”¨å¤±è´¥ -> è¿”å›é”™è¯¯ä¿¡æ¯
except Exception as e:
    return {
        "answer": f"æŠ±æ­‰ï¼Œå›ç­”ç”Ÿæˆå¤±è´¥ï¼š{str(e)}",
        "citations": [],
        "diagnostics": {"error": True}
    }
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### Token ä¼˜åŒ–æ•ˆæœ

```python
# ç¤ºä¾‹ç»Ÿè®¡
{
    "original_chunks": 15,           # åŸå§‹æ£€ç´¢åˆ°15ä¸ªå—
    "optimized_chunks": 8,           # ä¼˜åŒ–åä¿ç•™8ä¸ª
    "used_context_tokens": 28000,    # ä½¿ç”¨28k tokens
    "available_context_tokens": 28500, # å¯ç”¨28.5k tokens
    "tokens_prompt": 29000,          # æ€»prompt tokens
    "tokens_completion": 450,        # å›ç­” tokens
    "latency_ms": 3200              # å»¶è¿Ÿ3.2ç§’
}
```

### å‘é‡æ£€ç´¢æ€§èƒ½

```
HNSW ç´¢å¼•ï¼ˆ10ä¸‡æ¡æ•°æ®ï¼‰:
- TopK=5: ~10ms
- TopK=10: ~15ms
- TopK=20: ~25ms

æš´åŠ›æ£€ç´¢ï¼ˆ10ä¸‡æ¡æ•°æ®ï¼‰:
- TopK=5: ~500ms
- TopK=10: ~800ms
```

---

## ğŸ“ æ€»ç»“

**RAG ç³»ç»Ÿæ ¸å¿ƒä¼˜åŠ¿**ï¼š
1. âœ… **å‡†ç¡®æ€§**ï¼šåŸºäºå®é™…æ•°æ®å›ç­”ï¼Œé¿å…å¹»è§‰
2. âœ… **å¯è¿½æº¯**ï¼šæä¾›å¼•ç”¨æ¥æºï¼Œç”¨æˆ·å¯éªŒè¯
3. âœ… **å®æ—¶æ€§**ï¼šæ£€ç´¢æœ€æ–°æ•°æ®ï¼Œä¸å—æ¨¡å‹è®­ç»ƒæ—¶é—´é™åˆ¶
4. âœ… **å¯æ‰©å±•**ï¼šæ”¯æŒå¤šç§ LLM å’Œå‘é‡æ¨¡å‹

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š
- ğŸ” å‘é‡æ£€ç´¢ï¼ˆpgvector + HNSWï¼‰
- ğŸ¯ Token ç®¡ç†ï¼ˆç¡®ä¿ä¸è¶…é™ï¼‰
- ğŸ”„ æµå¼è¾“å‡ºï¼ˆSSEï¼‰
- ğŸ“¦ å¤šæ¨¡å¼æ”¯æŒï¼ˆtopic/globalï¼‰
- ğŸ›¡ï¸ é™çº§ç­–ç•¥ï¼ˆå¤±è´¥å‹å¥½å¤„ç†ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… çŸ¥è¯†é—®ç­”
- âœ… æ–‡æ¡£æ£€ç´¢
- âœ… å†…å®¹æ€»ç»“
- âœ… å¤šæºä¿¡æ¯æ•´åˆ

---

## ğŸ’¡ å‰ç«¯äº¤äº’ä¼˜åŒ–

### 1. **å¯¹è¯æ¨¡å¼ä¼˜åŒ–**

**äº‹ä»¶æ¨¡å¼ï¼ˆEvent Modeï¼‰**ï¼š
- ä¸“æ³¨äºäº‹ä»¶æ—¶é—´çº¿å±•ç¤º
- éšè—è¾“å…¥æ¡†ï¼Œåªæ˜¾ç¤ºæ—¶é—´çº¿
- ç‚¹å‡»äº‹ä»¶è‡ªåŠ¨åˆ·æ–°å¯¹è¯å¹¶å±•ç¤ºæ—¶é—´çº¿
- é€‚åˆå¿«é€Ÿæµè§ˆäº‹ä»¶è¯¦æƒ…

**è‡ªç”±æ¨¡å¼ï¼ˆFree Modeï¼‰**ï¼š
- å…¨å±€æ£€ç´¢å’Œé—®ç­”
- æ˜¾ç¤ºè¾“å…¥æ¡†ï¼Œæ”¯æŒè‡ªç”±æé—®
- åŸºäºå…¨å±€æ•°æ®æä¾›ç­”æ¡ˆ
- é€‚åˆè·¨äº‹ä»¶çš„ä¿¡æ¯æŸ¥è¯¢

```typescript
// é»˜è®¤ä¸ºäº‹ä»¶æ¨¡å¼
const [mode, setMode] = useState<"free" | "event">("event");

// æ¡ä»¶æ¸²æŸ“è¾“å…¥æ¡†
{mode === "free" && (
  <div className="conversation-input">
    <textarea ... />
    <button>å‘é€</button>
  </div>
)}
```

### 2. **äº¤äº’åŠŸèƒ½**

**åˆ·æ–°æŒ‰é’®**ï¼š
- ä½ç½®ï¼šå¯¹è¯æ ‡é¢˜å³ä¾§
- åŠŸèƒ½ï¼šé‡ç½®å¯¹è¯åˆ°åˆå§‹çŠ¶æ€
- å¿«æ·æ“ä½œï¼šæ¸…ç©ºè¾“å…¥æ¡†å’Œæ¶ˆæ¯å†å²

**Enterå‘é€**ï¼š
- Enteré”®ï¼šå‘é€æ¶ˆæ¯
- Shift+Enterï¼šæ¢è¡Œ
- æå‡è¾“å…¥æ•ˆç‡

```typescript
const handleTextareaKeyDown = (event: KeyboardEvent<HTMLTextAreaElement>) => {
  if (event.key === "Enter" && !event.shiftKey) {
    event.preventDefault();
    handleSubmit();
  }
};
```

**è‡ªåŠ¨æ»šåŠ¨**ï¼š
- æ–°æ¶ˆæ¯å‡ºç°æ—¶è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
- æµå¼è¾“å‡ºæ—¶å®æ—¶æ»šåŠ¨
- æä¾›æµç•…çš„å¯¹è¯ä½“éªŒ

```typescript
const messagesEndRef = useRef<HTMLDivElement | null>(null);

const scrollToBottom = () => {
  messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
};

useEffect(() => {
  scrollToBottom();
}, [messages]);
```

### 3. **UI/UX è®¾è®¡**

**ç»ç’ƒæ‹Ÿæ€è®¾è®¡ï¼ˆGlassmorphismï¼‰**ï¼š
```css
.conversation-input {
  background: linear-gradient(
    150deg, 
    rgba(30, 41, 59, 0.78), 
    rgba(15, 23, 42, 0.84)
  );
  backdrop-filter: blur(18px);
  border: 1px solid rgba(148, 163, 184, 0.18);
  box-shadow: 0 24px 55px rgba(15, 23, 42, 0.4);
}
```

**ç´§å‡‘å¸ƒå±€**ï¼š
- å‡å°‘æ—¶é—´çº¿å¡ç‰‡é—´è·ï¼ˆgap: 12px â†’ 8pxï¼‰
- ä¼˜åŒ–å†…è¾¹è·ï¼ˆpadding: 20px â†’ 16pxï¼‰
- ç¼©å°å­—å·ï¼ˆ14px â†’ 13pxï¼‰
- æå‡ä¿¡æ¯å¯†åº¦ï¼Œå‡å°‘æ»šåŠ¨

**è§†è§‰ç»Ÿä¸€**ï¼š
- æ ‡é¢˜åŠ ç²—ç»Ÿä¸€ï¼ˆfont-weight: 700ï¼‰
- å­—å·ç»Ÿä¸€ï¼ˆ20pxï¼‰
- æ•°æ®æºæ–‡å­—ä¸åŠ ç²—ï¼ˆfont-weight: 400ï¼‰
- ä¿æŒè§†è§‰å±‚çº§æ¸…æ™°

